{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab535b2",
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'https://kkb-production.jupyter-proxy.kaggle.net/'. Verify the server is running and reachable. (Failed to connect to the remote Jupyter Server 'https://kkb-production.jupyter-proxy.kaggle.net/'. Verify the server is running and reachable. (Invalid response: 404 Not Found).)."
     ]
    }
   ],
   "source": [
    "%%writefile transpose_v1.cu\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void matrix_transpose_kernel(const float* input, float* output, int rows, int cols) {\n",
    "    int row = blockDim.y*blockIdx.y+ threadIdx.y;\n",
    "    int col = blockDim.x*blockIdx.x+ threadIdx.x;\n",
    "    if (row<rows && col<cols)\n",
    "    {\n",
    "        output[col*rows+row] = input[row*cols+col];\n",
    "    }\n",
    "}\n",
    "\n",
    "// input, output are device pointers (i.e. pointers to memory on the GPU)\n",
    "extern \"C\" void solve(const float* input, float* output, int rows, int cols) {\n",
    "    dim3 threadsPerBlock(16, 16);\n",
    "    dim3 blocksPerGrid((cols + threadsPerBlock.x - 1) / threadsPerBlock.x,\n",
    "                       (rows + threadsPerBlock.y - 1) / threadsPerBlock.y);\n",
    "\n",
    "    matrix_transpose_kernel<<<blocksPerGrid, threadsPerBlock>>>(input, output, rows, cols);\n",
    "    cudaDeviceSynchronize();\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc4dbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -shared -o libtranspose_v1.so transpose_v1.cu -Xcompiler -fPIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d074e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在进行矩阵转置: 1024x512\n",
      "✅ 测试通过！结果正确。\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import ctypes\n",
    "import numpy as np\n",
    "\n",
    "# 1. 加载编译好的 .so 库\n",
    "lib = ctypes.CDLL('./libtranspose_v1.so')\n",
    "\n",
    "# 2. 定义函数参数类型\n",
    "# void solve(const float* A, const float* B, float* C, int M, int N, int K)\n",
    "# 指针对应 c_void_p (因为我们要传显存地址), int 对应 c_int\n",
    "lib.solve.argtypes = [\n",
    "    ctypes.c_void_p, \n",
    "    ctypes.c_void_p,\n",
    "    ctypes.c_int, \n",
    "    ctypes.c_int\n",
    "]\n",
    "\n",
    "def cuda_transpose(a_tensor):\n",
    "    # 获取维度\n",
    "    M, N = a_tensor.shape\n",
    "    \n",
    "    # 初始化输出矩阵 C (在 GPU 上分配)\n",
    "    c_tensor = torch.zeros((N, M), device='cuda', dtype=torch.float32)\n",
    "    \n",
    "    # 确保输入是连续内存且在 GPU 上\n",
    "    if not a_tensor.is_contiguous(): a_tensor = a_tensor.contiguous()\n",
    "    \n",
    "    # 3. 调用 CUDA 函数\n",
    "    # 注意：必须传入 data_ptr()，这是物理显存地址\n",
    "    lib.solve(\n",
    "        ctypes.c_void_p(a_tensor.data_ptr()),\n",
    "        ctypes.c_void_p(c_tensor.data_ptr()),\n",
    "        ctypes.c_int(M),\n",
    "        ctypes.c_int(N),\n",
    "    )\n",
    "    \n",
    "    return c_tensor\n",
    "\n",
    "# --- 测试部分 ---\n",
    "\n",
    "# 设置维度\n",
    "M, N = 1024, 512\n",
    "\n",
    "print(f\"正在进行矩阵转置: {M}x{N}\")\n",
    "\n",
    "# 创建随机数据 (在 GPU 上)\n",
    "A = torch.randn(M, N, device='cuda', dtype=torch.float32)\n",
    "\n",
    "# 1. 运行你的 CUDA Kernel\n",
    "C_custom = cuda_transpose(A)\n",
    "\n",
    "# 2. 运行 PyTorch 内置矩阵乘法 (作为标准答案)\n",
    "C_torch = torch.transpose(A, -2, -1)\n",
    "\n",
    "# 3. 验证结果\n",
    "# 允许一点浮点误差\n",
    "if torch.allclose(C_custom, C_torch, atol=1e-3):\n",
    "    print(\"✅ 测试通过！结果正确。\")\n",
    "else:\n",
    "    print(\"❌ 测试失败。结果不一致。\")\n",
    "    print(C_custom)\n",
    "    print(\"最大误差:\", (C_custom - C_torch).abs().max().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9133ea94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.7 µs ± 60.8 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
      "8.72 µs ± 109 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit C_custom = cuda_transpose(A); torch.cuda.synchronize()\n",
    "\n",
    "%timeit C_torch = torch.transpose(A, -2, -1); torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da636fc",
   "metadata": {},
   "source": [
    "上面的矩阵转置算子有很大的问题： 在input里面是一行一行的取数据的，但是在output里面是一列一列的放数据的。\n",
    "\n",
    "1. 取一行数据可以触发内存合并，连续的线程取连续的数据可以合并为一起请求。速度非常快\n",
    "2. 但是output里面放数据中，连续的线程往不连续的内存里面放数据，并且步长为rows，造成warp冲突，原本可以并行访问的共享内存现在只能串行访问。\n",
    "\n",
    "可视化代码： https://gemini.google.com/share/d587562e89e7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8978a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile transpose_v2.cu\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void matrix_transpose_kernel(const float* input, float* output, int rows, int cols) {\n",
    "    int row = blockDim.y*blockIdx.y+ threadIdx.y;\n",
    "    int col = blockDim.x*blockIdx.x+ threadIdx.x;\n",
    "    if (row<rows && col<cols)\n",
    "    {\n",
    "        output[col*rows+row] = input[row*cols+col];\n",
    "    }\n",
    "}\n",
    "\n",
    "// input, output are device pointers (i.e. pointers to memory on the GPU)\n",
    "extern \"C\" void solve(const float* input, float* output, int rows, int cols) {\n",
    "    dim3 threadsPerBlock(16, 16);\n",
    "    dim3 blocksPerGrid((cols + threadsPerBlock.x - 1) / threadsPerBlock.x,\n",
    "                       (rows + threadsPerBlock.y - 1) / threadsPerBlock.y);\n",
    "\n",
    "    matrix_transpose_kernel<<<blocksPerGrid, threadsPerBlock>>>(input, output, rows, cols);\n",
    "    cudaDeviceSynchronize();\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
