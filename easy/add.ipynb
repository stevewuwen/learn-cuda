{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9773f278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing add_kernel.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile add_kernel.cu\n",
    "#include <torch/extension.h>\n",
    "#include <cuda.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// CUDA 核函数\n",
    "template <typename scalar_t>\n",
    "__global__ void my_add_kernel(const scalar_t* __restrict__ input,\n",
    "                              scalar_t* __restrict__ output,\n",
    "                              int size) {\n",
    "    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (index < size) {\n",
    "        output[index] = input[index] + 1.0;\n",
    "    }\n",
    "}\n",
    "\n",
    "// C++ 调用的启动函数\n",
    "void my_add_cuda_launcher(const torch::Tensor& input, torch::Tensor& output) {\n",
    "    const int threads = 1024;\n",
    "    const int blocks = (input.numel() + threads - 1) / threads;\n",
    "\n",
    "    // AT_DISPATCH_FLOATING_TYPES 宏用于自动处理 float/double 类型分发\n",
    "    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"my_add_kernel\", ([&] {\n",
    "        my_add_kernel<scalar_t><<<blocks, threads>>>(\n",
    "            input.data_ptr<scalar_t>(),\n",
    "            output.data_ptr<scalar_t>(),\n",
    "            input.numel());\n",
    "    }));\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "326f0ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing add_op.cpp\n"
     ]
    }
   ],
   "source": [
    "%%writefile add_op.cpp\n",
    "#include <torch/extension.h>\n",
    "\n",
    "// 声明 CUDA launcher 函数\n",
    "void my_add_cuda_launcher(const torch::Tensor& input, torch::Tensor& output);\n",
    "\n",
    "// C++ 包装函数：检查并调用 CUDA\n",
    "torch::Tensor my_add(torch::Tensor input) {\n",
    "    // 检查输入是否在 CUDA 上\n",
    "    TORCH_CHECK(input.is_cuda(), \"Input tensor must be a CUDA tensor\");\n",
    "    // 检查内存是否连续\n",
    "    TORCH_CHECK(input.is_contiguous(), \"Input tensor must be contiguous\");\n",
    "\n",
    "    auto output = torch::zeros_like(input);\n",
    "    my_add_cuda_launcher(input, output);\n",
    "    return output;\n",
    "}\n",
    "\n",
    "// PyBind11 绑定模块\n",
    "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n",
    "    m.def(\"add_one\", &my_add, \"My custom add one operator\");\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fb1ae98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+1 算子功能正常\n",
      "Output: tensor([ 0.6145, -0.3479,  1.6543,  0.3850,  0.5882], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.cpp_extension import load\n",
    "\n",
    "# 编译并加载模块\n",
    "# name: 编译后的模块名\n",
    "# sources: 源文件列表\n",
    "my_extension = load(\n",
    "    name=\"add_op\",\n",
    "    sources=[\"add_op.cpp\", \"add_kernel.cu\"],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# 测试\n",
    "input_tensor = torch.randn(5, device=\"cuda\")\n",
    "pytorch_output = input_tensor+1\n",
    "\n",
    "# 调用算子\n",
    "output_tensor = my_extension.add_one(input_tensor)\n",
    "\n",
    "if (pytorch_output-output_tensor).abs().max() <0.1:\n",
    "    print(\"+1 算子功能正常\")\n",
    "    print(\"Output:\", output_tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
