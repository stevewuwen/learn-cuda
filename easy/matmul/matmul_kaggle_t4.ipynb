{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d0196f2",
   "metadata": {},
   "source": [
    "# 矩阵乘法\n",
    "\n",
    "在cuda c++中实现矩阵乘法（General Matrix Multiplication， GEMM）很容易，通过每一个线程在线程层级的索引，找到应该处理的A矩阵的行和B矩阵的列进行相乘求和，最后再写回C矩阵即可。下面就是最原始的 GEMM 实现方法。\n",
    "\n",
    "## 参考\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/657632577\n",
    "\n",
    "## gemm 数学定义与问题设定\n",
    "\n",
    "计算 $C = A \\times B + C$。\n",
    "假设矩阵维度：\n",
    "*   $A$: $M \\times K$\n",
    "*   $B$: $K \\times N$\n",
    "*   $C$: $M \\times N$\n",
    "\n",
    "在 CPU 上，这是一个三层循环：\n",
    "```cpp\n",
    "for (int i = 0; i < M; ++i)\n",
    "    for (int j = 0; j < N; ++j)\n",
    "        for (int k = 0; k < K; ++k)\n",
    "            C[i][j] += A[i][k] * B[k][j];\n",
    "```\n",
    "\n",
    "## 实现\n",
    "\n",
    "### v1 naive\n",
    "\n",
    "**核心思想**：GPU 拥有成千上万个线程。让**每个线程负责计算 C 矩阵中的一个元素**。\n",
    "\n",
    "*   **Grid (网格)** 覆盖整个 $M \\times N$ 的区域。\n",
    "*   **Block (线程块)** 大小通常设为 $16 \\times 16$。\n",
    "\n",
    "每一个线程会根据自己在线程层级的位置，去加载A中对应的一行，B中对应的一列来计算：\n",
    "```cpp\n",
    "for(int i = 0; i < N; i++){\n",
    "    acc += A[row * N + i] * B[i * K + col];\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922b3758",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile v1_naive.cu\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "__global__ void matrix_multiplication_kernel(const float* A, const float* B, float* C, int M, int N, int K) {\n",
    "    int row =  blockDim.y * blockIdx.y + threadIdx.y;\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    if(col >= K || row >= M){\n",
    "        return;\n",
    "    }\n",
    "    \n",
    "    float acc = 0.0f;\n",
    "    for(int i = 0; i < N; i++){\n",
    "        acc += A[row * N + i] * B[i * K + col];\n",
    "    }\n",
    "    C[row * K + col] = acc; \n",
    "}\n",
    "\n",
    "// 宿主端 wrapper 函数\n",
    "extern \"C\" void solve(const float* A, const float* B, float* C, int M, int N, int K) {\n",
    "    dim3 threadsPerBlock(16, 16);\n",
    "    dim3 blocksPerGrid((K + threadsPerBlock.x - 1) / threadsPerBlock.x,\n",
    "                       (M + threadsPerBlock.y - 1) / threadsPerBlock.y);\n",
    "    \n",
    "    matrix_multiplication_kernel<<<blocksPerGrid, threadsPerBlock>>>(A, B, C, M, N, K);\n",
    "    \n",
    "    // 检查是否有错误发生\n",
    "    cudaError_t err = cudaGetLastError();\n",
    "    if (err != cudaSuccess) {\n",
    "        printf(\"CUDA Error: %s\\n\", cudaGetErrorString(err));\n",
    "    }\n",
    "    cudaDeviceSynchronize();\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99606131",
   "metadata": {},
   "source": [
    "### v2 shared\n",
    "\n",
    "1. 使用共享内存\n",
    "   *  现状：在每一个线程里面，为了计算C里面的一个位置上面的值，需要去全局内存上访问N次数据A，也需要访问N次数据B，导致数据等待时间大大增加。\n",
    "   *  解决：使用线程块里面的共享内存，在一个线程块里面，先读取A和B的一部分数据到线程块里面的共享内存里面，然后进行计算，接着读取下一部分。通过把全局内存分块到共享内存上避免所有线程去读取全局内存里面的数据，这样只需要读取一次全局内存，剩下线程块里面的线程只需要去共享内存上面进行访问即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98cae66",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile v2_shared.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#define TILE_WIDTH 32\n",
    "\n",
    "__global__ void matrix_multiplication_shared_mem(const float* __restrict__ A, const float* __restrict__ B, float* C, int M, int N, int K) {\n",
    "\n",
    "    __shared__ float As[TILE_WIDTH][TILE_WIDTH];\n",
    "    __shared__ float Bs[TILE_WIDTH][TILE_WIDTH];\n",
    "\n",
    "    int bx = blockIdx.x;\n",
    "    int by = blockIdx.y;\n",
    "    int tx = threadIdx.x;\n",
    "    int ty = threadIdx.y;\n",
    "\n",
    "    int row = by * TILE_WIDTH + ty;\n",
    "    int col = bx * TILE_WIDTH + tx;\n",
    "\n",
    "    float acc = 0.0f;\n",
    "\n",
    "    // 循环遍历所有的 Tile\n",
    "    for (int t = 0; t < (N + TILE_WIDTH - 1) / TILE_WIDTH; ++t) {\n",
    "\n",
    "        if (row < M && t * TILE_WIDTH + tx < N) {\n",
    "            As[ty][tx] = A[row * N + t * TILE_WIDTH + tx];\n",
    "        } else {\n",
    "            As[ty][tx] = 0.0f;\n",
    "        }\n",
    "        if (col < K && t * TILE_WIDTH + ty < N) {\n",
    "            // 注意这里 B 的索引：行是 t*TILE_WIDTH + ty, 列是 col\n",
    "            Bs[ty][tx] = B[(t * TILE_WIDTH + ty) * K + col];\n",
    "        } else {\n",
    "            Bs[ty][tx] = 0.0f;\n",
    "        }\n",
    "        __syncthreads();\n",
    "        for (int i = 0; i < TILE_WIDTH; ++i) {\n",
    "            acc += As[ty][i] * Bs[i][tx];\n",
    "        }\n",
    "\n",
    "        __syncthreads();\n",
    "    }\n",
    "\n",
    "    if (row < M && col < K) {\n",
    "        C[row * K + col] = acc;\n",
    "    }\n",
    "}\n",
    "\n",
    "extern \"C\" void solve(const float* A, const float* B, float* C, int M, int N, int K) {\n",
    "    dim3 threadsPerBlock(TILE_WIDTH, TILE_WIDTH);\n",
    "    dim3 blocksPerGrid((K + threadsPerBlock.x - 1) / threadsPerBlock.x,\n",
    "                       (M + threadsPerBlock.y - 1) / threadsPerBlock.y);\n",
    "    \n",
    "    matrix_multiplication_shared_mem<<<blocksPerGrid, threadsPerBlock>>>(A, B, C, M, N, K);\n",
    "    \n",
    "    cudaError_t err = cudaGetLastError();\n",
    "    if (err != cudaSuccess) {\n",
    "        printf(\"CUDA Error: %s\\n\", cudaGetErrorString(err));\n",
    "    }\n",
    "    \n",
    "    cudaDeviceSynchronize();\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9034e71b",
   "metadata": {},
   "source": [
    "### v3 vector load & thread coarsening\n",
    "\n",
    "1. 向量化加载（vector load）\n",
    "    *   问题： 每次线程在从全局内存上面加载数据到共享内存时，或者在共享内存上面加载数据到寄存器时，通常只加载一个float数据，通常是4个字节。虽然有访存合并，但是一次加载和存放的数据太小了。\n",
    "    *   解决：让一个线程一次性读取多个数据元素，将其视为一个向量进行传输。CUDA 为此提供了内置的向量数据类型，如 `int2`, `int4`, `float2`, `float4`, `double2` 等。\n",
    "\n",
    "2. 线程粗化（thread coarsening）\n",
    "    * 问题：线程块里面相邻的线程往往会加载相同的数据进行计算，不仅造成指令冗余，还多了很多不必要的等待时间。此外，一个线程做的事太少了，加载的数据和进行计算的数据都太小了，无法有效利用指令级并行和内存级并行\n",
    "    * 解决：让一个线程加载更多的数据，处理更多的数据。让一个线程同时加载更多数据到寄存器上面进行计算：\n",
    "        ```cpp\n",
    "            // 1. 将所需的 As 和 Bs 数据预加载到寄存器\n",
    "            // 每个线程计算 4x4，需要 As 的一列 4 个值，Bs 的一行 4 个值\n",
    "            for (int i = 0; i < WPT; ++i) {\n",
    "                reg_A[i] = As[ty * WPT + i][k];\n",
    "                reg_B[i] = Bs[k][tx * WPT + i];\n",
    "            }\n",
    "\n",
    "            // 2. 外积计算 (Outer Product)\n",
    "            // 计算 4x4 的结果，复用 reg_A 和 reg_B\n",
    "            for (int row = 0; row < WPT; ++row) {\n",
    "                for (int col = 0; col < WPT; ++col) {\n",
    "                    accum[row][col] += reg_A[row] * reg_B[col];\n",
    "                }\n",
    "            }\n",
    "        ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8790ea4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile v3_vl_tc.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// 宏定义块大小\n",
    "// TS (Tile Size): 每个 Block 计算 64x64 的 C\n",
    "// WPT (Work Per Thread): 每个线程计算 4x4 的 C\n",
    "// TS_K: K 维度(你的代码里是 N 维度)的分块大小，设为 8 或 16\n",
    "#define TS 64\n",
    "#define WPT 4\n",
    "#define TS_K 16 \n",
    "\n",
    "// 优化后的 Kernel\n",
    "__global__ void matrix_multiplication_optimized(\n",
    "    const float* __restrict__ A, \n",
    "    const float* __restrict__ B, \n",
    "    float* __restrict__ C, \n",
    "    int M, int N, int K) \n",
    "{\n",
    "    // 每个 Block 处理 C 中 TS x TS (64x64) 的区域\n",
    "    // 线程块维度: dim3(TS/WPT, TS/WPT) -> (16, 16) -> 256 个线程\n",
    "    \n",
    "    // 1. 声明共享内存\n",
    "    // As: 存储 A 的切片 [TS][TS_K] -> [64][16]\n",
    "    // Bs: 存储 B 的切片 [TS_K][TS] -> [16][64]\n",
    "    __shared__ float As[TS][TS_K];\n",
    "    __shared__ float Bs[TS_K][TS];\n",
    "\n",
    "    // 2. 声明寄存器\n",
    "    // accum: 累加器，每个线程负责计算 4x4 = 16 个元素\n",
    "    float accum[WPT][WPT] = {0.0f};\n",
    "    \n",
    "    // reg_A, reg_B: 用于在内循环中缓存从 SMEM 读取的值\n",
    "    float reg_A[WPT];\n",
    "    float reg_B[WPT];\n",
    "\n",
    "    // 线程 ID 和 Block ID\n",
    "    int tx = threadIdx.x; // range 0-15\n",
    "    int ty = threadIdx.y; // range 0-15\n",
    "    int bx = blockIdx.x;\n",
    "    int by = blockIdx.y;\n",
    "\n",
    "    // 当前线程负责的 C 矩阵起始坐标 (C 的分块左上角 + 线程偏移)\n",
    "    // 每个线程覆盖 WPT(4) 个像素宽/高\n",
    "    int row_c = by * TS + ty * WPT; \n",
    "    int col_c = bx * TS + tx * WPT;\n",
    "\n",
    "    // 3. 循环遍历 N 维度 (步长 TS_K = 16)\n",
    "    for (int t = 0; t < N; t += TS_K) {\n",
    "        \n",
    "        // --- 加载数据到 Shared Memory (协作加载) ---\n",
    "        // 我们有 256 个线程。\n",
    "        // 需要加载 A 的 Tile: 64行 * 16列 = 1024 元素。每个线程加载 1024/256 = 4 个元素。\n",
    "        // 需要加载 B 的 Tile: 16行 * 64列 = 1024 元素。每个线程加载 4 个元素。\n",
    "        \n",
    "        // 加载 As (A 的子块): \n",
    "        // 这里的逻辑是将 256 个线程映射到 64x16 的区域\n",
    "        // 我们使用 float4 向量化加载来极致优化带宽\n",
    "        \n",
    "        // 计算当前线程加载 As 的位置\n",
    "        // 将 16x16 的线程块视为 256 个线性线程\n",
    "        int tid = ty * (TS / WPT) + tx; // 0 ~ 255\n",
    "        \n",
    "        // 映射到 As[64][16]: 每一行 16 个元素，如果是 float4 就是 4 个 float4\n",
    "        // 256 个线程，每个加载 1 个 float4 (4个float)，正好 1024 个 float\n",
    "        // As 的行索引\n",
    "        int load_a_row = tid / (TS_K / 4); \n",
    "        int load_a_col = (tid % (TS_K / 4)) * 4;\n",
    "        \n",
    "        // 从全局内存 A 加载到 As\n",
    "        // 全局索引: A[(by * TS + load_a_row) * N + (t + load_a_col)]\n",
    "        // 注意边界检查省略了，假设维度对其\n",
    "        if (by * TS + load_a_row < M && t + load_a_col < N) {\n",
    "             // 使用 float4 指针强转进行向量加载\n",
    "             float4 tmp = reinterpret_cast<const float4*>(&A[(by * TS + load_a_row) * N + (t + load_a_col)])[0];\n",
    "             As[load_a_row][load_a_col + 0] = tmp.x;\n",
    "             As[load_a_row][load_a_col + 1] = tmp.y;\n",
    "             As[load_a_row][load_a_col + 2] = tmp.z;\n",
    "             As[load_a_row][load_a_col + 3] = tmp.w;\n",
    "        }\n",
    "\n",
    "        // 加载 Bs (B 的子块): [16][64]\n",
    "        // 同样用 tid 映射。每行 64 个元素 = 16 个 float4。\n",
    "        // 总共 16 行。总 float4 数 = 16 * 16 = 256。正好每个线程取 1 个 float4。\n",
    "        int load_b_row = tid / (TS / 4);\n",
    "        int load_b_col = (tid % (TS / 4)) * 4;\n",
    "\n",
    "        if (t + load_b_row < N && bx * TS + load_b_col < K) {\n",
    "             float4 tmp = reinterpret_cast<const float4*>(&B[(t + load_b_row) * K + (bx * TS + load_b_col)])[0];\n",
    "             Bs[load_b_row][load_b_col + 0] = tmp.x;\n",
    "             Bs[load_b_row][load_b_col + 1] = tmp.y;\n",
    "             Bs[load_b_row][load_b_col + 2] = tmp.z;\n",
    "             Bs[load_b_row][load_b_col + 3] = tmp.w;\n",
    "        }\n",
    "\n",
    "        __syncthreads(); // 等待数据加载完成\n",
    "\n",
    "        // --- 在寄存器上进行计算 ---\n",
    "        // 遍历 Shared Memory 中的 TS_K (16) 维度\n",
    "        #pragma unroll\n",
    "        for (int k = 0; k < TS_K; ++k) {\n",
    "            \n",
    "            // 1. 将所需的 As 和 Bs 数据预加载到寄存器\n",
    "            // 每个线程计算 4x4，需要 As 的一列 4 个值，Bs 的一行 4 个值\n",
    "            for (int i = 0; i < WPT; ++i) {\n",
    "                reg_A[i] = As[ty * WPT + i][k];\n",
    "                reg_B[i] = Bs[k][tx * WPT + i];\n",
    "            }\n",
    "\n",
    "            // 2. 外积计算 (Outer Product)\n",
    "            // 计算 4x4 的结果，复用 reg_A 和 reg_B\n",
    "            for (int row = 0; row < WPT; ++row) {\n",
    "                for (int col = 0; col < WPT; ++col) {\n",
    "                    accum[row][col] += reg_A[row] * reg_B[col];\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        __syncthreads(); // 等待计算完成，准备加载下一块\n",
    "    }\n",
    "\n",
    "    // 4. 写回结果到全局内存\n",
    "    // 每个线程写回 4x4 个点\n",
    "    for (int row = 0; row < WPT; ++row) {\n",
    "        for (int col = 0; col < WPT; ++col) {\n",
    "            int global_row = row_c + row;\n",
    "            int global_col = col_c + col;\n",
    "            \n",
    "            if (global_row < M && global_col < K) {\n",
    "                C[global_row * K + global_col] = accum[row][col];\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "// Host 端调用示例\n",
    "extern \"C\" void solve(const float* d_A, const float* d_B, float* d_C, int M, int N, int K) {\n",
    "    // 线程块大小: 16x16 = 256 线程\n",
    "    dim3 threadsPerBlock(TS / WPT, TS / WPT); \n",
    "    \n",
    "    // Grid 大小: 因为每个 Block 处理 64x64，所以除以 TS(64)\n",
    "    dim3 numBlocks((K + TS - 1) / TS, (M + TS - 1) / TS);\n",
    "\n",
    "    matrix_multiplication_optimized<<<numBlocks, threadsPerBlock>>>(d_A, d_B, d_C, M, N, K);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600d766d",
   "metadata": {},
   "source": [
    "### v3_1 scale\n",
    "\n",
    "1. 增大线程处理的数据\n",
    "    * 问题： v3 代码中1个线程只处理 4*4 个输出数据，没有充分利用线程上面的寄存器和shared memory\n",
    "    * 解决： 因此可以让一个线程处理更多的数据。下面这个版本一个线程可以处理`8*8`个数据\n",
    "    * 注意： 线程处理的数据不能无限大，因为寄存器的大小是有限的。如果往寄存器里面加载的数据过多，不仅会导致寄存器溢出，也就是寄存器里面的值被卸载到shared memory上面去，还会导致sm上面的block数据减少，降低了占用率，使并发量降低。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9484a9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile v3_1_vl_tc.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// 宏定义块大小\n",
    "// TS (Tile Size): 每个 Block 计算 128x128 的 C\n",
    "// WPT (Work Per Thread): 每个线程计算 8x8 的 C\n",
    "// TS_K: 共享内存中的分块大小\n",
    "#define TS 128\n",
    "#define WPT 8\n",
    "#define TS_K 16 \n",
    "\n",
    "__global__ void matrix_multiplication_optimized(\n",
    "    const float* __restrict__ A, \n",
    "    const float* __restrict__ B, \n",
    "    float* __restrict__ C, \n",
    "    int M, int N, int K) \n",
    "{\n",
    "    __shared__ float As[TS][TS_K];\n",
    "    __shared__ float Bs[TS_K][TS];\n",
    "\n",
    "    float accum[WPT][WPT] = {0.0f};\n",
    "    \n",
    "    float reg_A[WPT];\n",
    "    float reg_B[WPT];\n",
    "\n",
    "    int tx = threadIdx.x;\n",
    "    int ty = threadIdx.y;\n",
    "    int bx = blockIdx.x;\n",
    "    int by = blockIdx.y;\n",
    "    \n",
    "    // 当前线程负责的 C 矩阵起始坐标 (C 的分块左上角 + 线程偏移)\n",
    "    int row_c = by * TS + ty * WPT; \n",
    "    int col_c = bx * TS + tx * WPT;\n",
    "\n",
    "    for (int t = 0; t < N; t += TS_K) {\n",
    "        \n",
    "        int tid = ty * (TS / WPT) + tx;\n",
    "        \n",
    "        int load_a_row = tid / (TS_K / WPT); \n",
    "        int load_a_col = (tid % (TS_K / WPT)) * WPT;\n",
    "        \n",
    "        if (by * TS + load_a_row < M && t + load_a_col < N) {\n",
    "             // 使用 float4 指针强转进行向量加载\n",
    "             // 1. 计算内存中的基础偏移量 (base index)\n",
    "            int base_offset = (by * TS + load_a_row) * N + (t + load_a_col);\n",
    "\n",
    "            // 2. 将指针强转为 float4* 并读取两个连续的 float4\n",
    "            // A[base_offset] 是起点\n",
    "            const float4* A_ptr = reinterpret_cast<const float4*>(&A[base_offset]);\n",
    "\n",
    "            float4 tmp1 = A_ptr[0]; // 加载第 0-15 字节 (float 0-3)\n",
    "            float4 tmp2 = A_ptr[1]; // 加载第 16-31 字节 (float 4-7)\n",
    "\n",
    "            // 3. 将数据写入共享内存 As\n",
    "            // 优化：同样使用 float4 向量化写入共享内存，比逐个 float 赋值更快\n",
    "            float4* As_ptr = reinterpret_cast<float4*>(&As[load_a_row][load_a_col]);\n",
    "\n",
    "            As_ptr[0] = tmp1; // 写入 As[row][col + 0~3]\n",
    "            As_ptr[1] = tmp2; // 写入 As[row][col + 4~7]\n",
    "        }\n",
    "\n",
    "        // 加载 Bs (B 的子块): [16][64]\n",
    "        // 同样用 tid 映射。每行 64 个元素 = 16 个 float4。\n",
    "        // 总共 16 行。总 float4 数 = 16 * 16 = 256。正好每个线程取 1 个 float4。\n",
    "        int load_b_row = tid / (TS / WPT);\n",
    "        int load_b_col = (tid % (TS / WPT)) * WPT;\n",
    "\n",
    "        if (t + load_b_row < N && bx * TS + load_b_col < K) {\n",
    "            // 使用 float4 指针强转进行向量加载\n",
    "            // 1. 计算内存中的基础偏移量 (base index)\n",
    "            int base_offset = (t + load_b_row) * K + (bx * TS + load_b_col);\n",
    "\n",
    "            // 2. 将指针强转为 float4* 并读取两个连续的 float4\n",
    "            // A[base_offset] 是起点\n",
    "            const float4* B_ptr = reinterpret_cast<const float4*>(&B[base_offset]);\n",
    "\n",
    "            float4 tmp1 = B_ptr[0]; // 加载第 0-15 字节 (float 0-3)\n",
    "            float4 tmp2 = B_ptr[1]; // 加载第 16-31 字节 (float 4-7)\n",
    "\n",
    "            // 3. 将数据写入共享内存 As\n",
    "            // 优化：同样使用 float4 向量化写入共享内存，比逐个 float 赋值更快\n",
    "            float4* Bs_ptr = reinterpret_cast<float4*>(&Bs[load_b_row][load_b_col]);\n",
    "\n",
    "            Bs_ptr[0] = tmp1; // 写入 As[row][col + 0~3]\n",
    "            Bs_ptr[1] = tmp2; // 写入 As[row][col + 4~7]\n",
    "        }\n",
    "\n",
    "        __syncthreads(); // 等待数据加载完成\n",
    "\n",
    "        // --- 在寄存器上进行计算 ---\n",
    "        // 遍历 Shared Memory 中的 TS_K (16) 维度\n",
    "        #pragma unroll\n",
    "        for (int k = 0; k < TS_K; ++k) {\n",
    "            \n",
    "            // 1. 将所需的 As 和 Bs 数据预加载到寄存器\n",
    "            // 每个线程计算 4x4，需要 As 的一列 4 个值，Bs 的一行 4 个值\n",
    "            for (int i = 0; i < WPT; ++i) {\n",
    "                reg_A[i] = As[ty * WPT + i][k];\n",
    "                reg_B[i] = Bs[k][tx * WPT + i];\n",
    "            }\n",
    "\n",
    "            // 2. 外积计算 (Outer Product)\n",
    "            // 计算 4x4 的结果，复用 reg_A 和 reg_B\n",
    "            for (int row = 0; row < WPT; ++row) {\n",
    "                for (int col = 0; col < WPT; ++col) {\n",
    "                    accum[row][col] += reg_A[row] * reg_B[col];\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        __syncthreads(); // 等待计算完成，准备加载下一块\n",
    "    }\n",
    "\n",
    "    // 4. 写回结果到全局内存\n",
    "    // 每个线程写回 4x4 个点\n",
    "    for (int row = 0; row < WPT; ++row) {\n",
    "        for (int col = 0; col < WPT; ++col) {\n",
    "            int global_row = row_c + row;\n",
    "            int global_col = col_c + col;\n",
    "            \n",
    "            if (global_row < M && global_col < K) {\n",
    "                C[global_row * K + global_col] = accum[row][col];\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "// Host 端调用示例\n",
    "extern \"C\" void solve(const float* d_A, const float* d_B, float* d_C, int M, int N, int K) {\n",
    "    // 线程块大小: 16x16 = 256 线程\n",
    "    dim3 threadsPerBlock(TS / WPT, TS / WPT); \n",
    "    \n",
    "    // Grid 大小: 因为每个 Block 处理 64x64，所以除以 TS(64)\n",
    "    dim3 numBlocks((K + TS - 1) / TS, (M + TS - 1) / TS);\n",
    "\n",
    "    matrix_multiplication_optimized<<<numBlocks, threadsPerBlock>>>(d_A, d_B, d_C, M, N, K);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6891f591",
   "metadata": {},
   "source": [
    "### v4 prefetch & vector back\n",
    "\n",
    "v3的版本还可以继续优化：\n",
    "\n",
    "1.  **数据预取**：\n",
    "    *   **现状**：v3_3的代码逻辑是 `Load -> Sync -> Compute -> Sync`。在 `Compute` 期间，内存总线是空闲的；在 `Load` 期间，计算单元（ALU）是空闲的。\n",
    "    *   **优化**：使用两个 Shared Memory 缓冲区。当 GPU 正在计算当前块（Tile $k$）时，同时将下一个块（Tile $k+1$）从 Global Memory 加载到寄存器，并在计算间隙写入另一个 Shared Memory 缓冲区。这可以掩盖内存延迟。引入了 __shared__ ... [2][...]。在主循环中，并没有直接从 Global Memory 写入 Shared Memory，而是先加载到 ldg_a_reg (Local Registers)。\n",
    "  \n",
    "    流水线：\n",
    "    1. Load Next Tile (Global -> Register)\n",
    "    2. Compute Current Tile (Shared -> Register -> ALU)\n",
    "    2. __syncthreads()\n",
    "    2. Store Next Tile (Register -> Shared)\n",
    "    2. __syncthreads()\n",
    "    \n",
    "    这种写法允许 GPU 在做乘加运算（耗时最长）的同时，利用内存控制器去抓取下一个块的数据。\n",
    "\n",
    "2.  **向量化回写 Global Memory**：\n",
    "    *   **现状**：最后写入 `C` 时使用了双重循环逐个 `float` 写入。\n",
    "    *   **优化**：同样使用 `float4` 指针转换，将计算结果向量化写回 Global Memory，提高写入带宽利用率。\n",
    "\n",
    "3.  **索引计算简化**：\n",
    "    *   **现状**：代码中的索引计算（如 `tid / (TS_K / WPT)`）涉及较多整数除法和取模，这些指令在 GPU 上比加减法慢。\n",
    "    *   **优化**：可以通过预计算或位运算优化。使用 `<` 和 `>` 等位运算来代替乘除法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c47582",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile v4_pre_vb.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// 宏定义块大小\n",
    "#define TS 128      // Tile Size M, N\n",
    "#define TS_K 16     // Tile Size K\n",
    "#define WPT 8       // Work Per Thread\n",
    "\n",
    "__global__ void matrix_multiplication_double_buffer(\n",
    "    const float* __restrict__ A, \n",
    "    const float* __restrict__ B, \n",
    "    float* __restrict__ C, \n",
    "    int M, int N, int K) \n",
    "{\n",
    "    // Double Buffering: 申请 2 倍的 Shared Memory\n",
    "    // [2] 代表两个缓冲区：buffer_curr 和 buffer_next\n",
    "    __shared__ float As[2][TS][TS_K];\n",
    "    __shared__ float Bs[2][TS_K][TS];\n",
    "\n",
    "    // 寄存器累加器\n",
    "    float accum[WPT][WPT] = {0.0f};\n",
    "    \n",
    "    // 用于计算的寄存器片段\n",
    "    float reg_A[WPT];\n",
    "    float reg_B[WPT];\n",
    "\n",
    "    // 用于预取 Global Memory 数据的寄存器\n",
    "    // 每个线程负责加载 8 个 float (2个 float4)\n",
    "    float ldg_a_reg[2][4]; // [2个float4][每个包含4个float]\n",
    "    float ldg_b_reg[2][4];\n",
    "\n",
    "    int tx = threadIdx.x;\n",
    "    int ty = threadIdx.y;\n",
    "    int bx = blockIdx.x;\n",
    "    int by = blockIdx.y;\n",
    "    \n",
    "    // 在线程块中取出这个线程的索引， TS/WPT 表示一行有多少个线程（写回的字节块宽度/每一个线程处理的字节数）\n",
    "    int tid = ty * (TS / WPT) + tx; // 0 ~ 255\n",
    "\n",
    "    // --- 1. 预计算加载索引 ---\n",
    "    // 线程负责加载 A 的位置: A_tile 是 128x16\n",
    "    // 每个线程加载 8 个元素。总共 256 线程 * 8 = 2048 元素 = 128*16。\n",
    "    // A 的行索引 (0-127) 和 列索引 (0, 8)\n",
    "    int load_a_row = tid >> 1; // tid / 2\n",
    "    int load_a_col = (tid & 1) << 3; // (tid % 2) * 8\n",
    "\n",
    "    // 线程负责加载 B 的位置: B_tile 是 16x128\n",
    "    // B 的行索引 (0-15) 和 列索引 (0-120，步长8)\n",
    "    int load_b_row = tid >> 4; // tid / 16\n",
    "    int load_b_col = (tid & 15) << 3; // (tid % 16) * 8\n",
    "\n",
    "    // A 和 B 在 Global Memory 中的基础指针（调整到当前 Block 的行/列起点）\n",
    "    // 注意：这里假设 A是(M, N/K_dim)，B是(N/K_dim, K/N_width)。\n",
    "    // 根据用户代码逻辑：\n",
    "    // A: [M][N_arg], B: [N_arg][K_arg] (这里 N_arg 对应 K 维度)\n",
    "    // 使用题目中的变量名：\n",
    "    // A 的行偏移由 by 决定，B 的列偏移由 bx 决定\n",
    "    const float* A_ptr_base = A + (by * TS + load_a_row) * N; \n",
    "    const float* B_ptr_base = B + load_b_row * K + (bx * TS + load_b_col);\n",
    "\n",
    "    // --- 2. Prologue: 加载第一个 Tile 到寄存器，然后放入 SMEM ---\n",
    "    \n",
    "    // 边界检查并加载 A\n",
    "    {\n",
    "        // 这里的 t=0，对应 A 的列偏移 load_a_col\n",
    "        bool row_valid = (by * TS + load_a_row < M);\n",
    "        // 使用 float4 加载\n",
    "        const float4* A_vec_ptr = reinterpret_cast<const float4*>(A_ptr_base + load_a_col);\n",
    "        // 只能在内存安全时读取，否则填 0\n",
    "        float4 tmp1 = (row_valid && (load_a_col < N)) ? A_vec_ptr[0] : make_float4(0.0f, 0.0f, 0.0f, 0.0f);\n",
    "        float4 tmp2 = (row_valid && (load_a_col + 4 < N)) ? A_vec_ptr[1] : make_float4(0.0f, 0.0f, 0.0f, 0.0f);\n",
    "        \n",
    "        // 写入 SMEM buffer 0\n",
    "        // 手动展开 float4 赋值以匹配 float 数组结构，或者转换指针\n",
    "        // 这里为了 Padding 对齐安全，建议逐个或 reinterpret_cast 写入\n",
    "        // As[0][load_a_row][load_a_col + 0..3]\n",
    "        reinterpret_cast<float4*>(&As[0][load_a_row][load_a_col])[0] = tmp1;\n",
    "        reinterpret_cast<float4*>(&As[0][load_a_row][load_a_col + 4])[0] = tmp2;\n",
    "    }\n",
    "\n",
    "    // 边界检查并加载 B\n",
    "    {\n",
    "        int col_B = bx * TS + load_b_col;\n",
    "        // t=0\n",
    "        const float4* B_vec_ptr = reinterpret_cast<const float4*>(B_ptr_base); \n",
    "        // 注意：B 是 Row-Major，B_ptr_base 是起点。\n",
    "        // 但这里 B 的 Tile 是随着 K 维度(用户变量 N) 移动的。\n",
    "        // 实际上 B 的指针移动是：ptr + t * K_width。\n",
    "        // 原代码逻辑：(t + load_b_row) * K\n",
    "        \n",
    "        // 修正指针计算：每次循环 B 向下移，A 向右移\n",
    "        const float* B_ptr_curr = B + load_b_row * K + col_B;\n",
    "        \n",
    "        float4 tmp1 = (load_b_row < N && col_B<K) ? \n",
    "                      reinterpret_cast<const float4*>(B_ptr_curr)[0] : make_float4(0.0f,0.0f,0.0f,0.0f);\n",
    "        float4 tmp2 = (load_b_row < N && (col_B + 4 < K)) ? \n",
    "                      reinterpret_cast<const float4*>(B_ptr_curr + 4)[0] : make_float4(0.0f,0.0f,0.0f,0.0f);\n",
    "\n",
    "        reinterpret_cast<float4*>(&Bs[0][load_b_row][load_b_col])[0] = tmp1;\n",
    "        reinterpret_cast<float4*>(&Bs[0][load_b_row][load_b_col + 4])[0] = tmp2;\n",
    "    }\n",
    "\n",
    "    __syncthreads();\n",
    "\n",
    "    // --- 3. Main Loop ---\n",
    "    int write_stage_idx = 1; // 下一次写入的 buffer\n",
    "    int load_stage_idx = 0;  // 当前计算使用的 buffer\n",
    "\n",
    "    // t 从 0 开始，每次步进 TS_K\n",
    "    // 注意：Prologue 已经加载了 t=0 的数据。\n",
    "    // 循环内主要做：计算(t)，预加载(t+TS_K)\n",
    "    for (int t = 0; t < N; t += TS_K) {\n",
    "        \n",
    "        // === Step 3.1: 预加载 下一个 Tile (t + TS_K) 到 寄存器 ===\n",
    "        // 这样不会覆盖当前正在被 Compute 读取的 SMEM\n",
    "        int next_t = t + TS_K;\n",
    "        \n",
    "        if (next_t < N) {\n",
    "            // Load A\n",
    "            bool row_valid = (by * TS + load_a_row < M);\n",
    "            // 指针偏移：A 向右移 TS_K\n",
    "            int load_next_a_col = next_t + load_a_col;\n",
    "            const float4* A_vec_ptr = reinterpret_cast<const float4*>(A_ptr_base + load_next_a_col);\n",
    "            \n",
    "            float4 t1 = (row_valid && (load_next_a_col < N)) ? A_vec_ptr[0] : make_float4(0.0f,0.0f,0.0f,0.0f);\n",
    "            float4 t2 = (row_valid && (load_next_a_col + 4 < N)) ? A_vec_ptr[1] : make_float4(0.0f,0.0f,0.0f,0.0f);\n",
    "            \n",
    "            // 暂存到寄存器\n",
    "            reinterpret_cast<float4*>(ldg_a_reg)[0] = t1;\n",
    "            reinterpret_cast<float4*>(ldg_a_reg)[1] = t2;\n",
    "\n",
    "            // Load B\n",
    "            int load_next_b_col = bx * TS + load_b_col;\n",
    "            int load_next_b_row = next_t + load_b_row;\n",
    "            // 指针偏移：B 向下移 TS_K\n",
    "            const float* B_ptr_next = B + load_next_b_row * K + load_next_b_col;\n",
    "            \n",
    "            float4 t3 = (load_next_b_row < N && load_next_b_col<K) ? \n",
    "                        reinterpret_cast<const float4*>(B_ptr_next)[0] : make_float4(0.0f,0.0f,0.0f,0.0f);\n",
    "            float4 t4 = (load_next_b_row < N && (load_next_b_col + 4 < K)) ? \n",
    "                        reinterpret_cast<const float4*>(B_ptr_next + 4)[0] : make_float4(0.0f,0.0f,0.0f,0.0f);\n",
    "            \n",
    "            reinterpret_cast<float4*>(ldg_b_reg)[0] = t3;\n",
    "            reinterpret_cast<float4*>(ldg_b_reg)[1] = t4;\n",
    "        }\n",
    "\n",
    "        // === Step 3.2: 计算 当前 Tile (SMEM[load_stage_idx]) ===\n",
    "        #pragma unroll\n",
    "        for (int k = 0; k < TS_K; ++k) {\n",
    "            // 加载 A 的一列 (由当前线程负责的 WPT 行)\n",
    "            #pragma unroll\n",
    "            for (int i = 0; i < WPT; ++i) {\n",
    "                // As 有 Padding，访问安全\n",
    "                reg_A[i] = As[load_stage_idx][ty * WPT + i][k];\n",
    "            }\n",
    "            // 加载 B 的一行 (由当前线程负责的 WPT 列)\n",
    "            #pragma unroll\n",
    "            for (int i = 0; i < WPT; ++i) {\n",
    "                reg_B[i] = Bs[load_stage_idx][k][tx * WPT + i];\n",
    "            }\n",
    "            // 外积计算\n",
    "            #pragma unroll\n",
    "            for (int row = 0; row < WPT; ++row) {\n",
    "                #pragma unroll\n",
    "                for (int col = 0; col < WPT; ++col) {\n",
    "                    accum[row][col] += reg_A[row] * reg_B[col];\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        // === Step 3.3: Store 寄存器 -> SMEM ===\n",
    "        // 必须先 Sync，确保所有线程都完成了 Compute (读 SMEM)，才能写入下一轮的数据\n",
    "        __syncthreads();\n",
    "\n",
    "        if (next_t < N) {\n",
    "            // 将预加载到寄存器的数据写入 Shared Memory 的另一半\n",
    "            reinterpret_cast<float4*>(&As[write_stage_idx][load_a_row][load_a_col])[0] = reinterpret_cast<float4*>(ldg_a_reg)[0];\n",
    "            reinterpret_cast<float4*>(&As[write_stage_idx][load_a_row][load_a_col + 4])[0] = reinterpret_cast<float4*>(ldg_a_reg)[1];\n",
    "\n",
    "            reinterpret_cast<float4*>(&Bs[write_stage_idx][load_b_row][load_b_col])[0] = reinterpret_cast<float4*>(ldg_b_reg)[0];\n",
    "            reinterpret_cast<float4*>(&Bs[write_stage_idx][load_b_row][load_b_col + 4])[0] = reinterpret_cast<float4*>(ldg_b_reg)[1];\n",
    "        }\n",
    "\n",
    "        // 交换 buffer索引\n",
    "        load_stage_idx = 1 - load_stage_idx;\n",
    "        write_stage_idx = 1 - write_stage_idx;\n",
    "\n",
    "        // 必须再次 Sync，确保 SMEM 写入完成，下一轮 Compute 可以读取\n",
    "        __syncthreads();\n",
    "    }\n",
    "\n",
    "    // --- 4. 写回结果到全局内存 (向量化) ---\n",
    "    // 每个线程负责 8x8 的块\n",
    "    // 我们可以每行写两个 float4\n",
    "    int row_c = by * TS + ty * WPT; \n",
    "    int col_c = bx * TS + tx * WPT;\n",
    "\n",
    "    #pragma unroll\n",
    "    for (int row = 0; row < WPT; ++row) {\n",
    "        int global_row = row_c + row;\n",
    "        if (global_row < M) {\n",
    "            #pragma unroll\n",
    "            for (int col_offset = 0; col_offset < WPT; col_offset += 4) {\n",
    "                int global_col = col_c + col_offset;\n",
    "                if (global_col < K) { // 简单边界检查，假设 K 是 4 的倍数\n",
    "                     // 构造 float4\n",
    "                     float4 res;\n",
    "                     res.x = accum[row][col_offset + 0];\n",
    "                     res.y = accum[row][col_offset + 1];\n",
    "                     res.z = accum[row][col_offset + 2];\n",
    "                     res.w = accum[row][col_offset + 3];\n",
    "                     \n",
    "                     // 写入\n",
    "                     if (global_col + 4 <= K) {\n",
    "                        reinterpret_cast<float4*>(&C[global_row * K + global_col])[0] = res;\n",
    "                     } else {\n",
    "                        // 边缘处理 (略，假设对其)\n",
    "                        for(int i=0; i<4; i++) {\n",
    "                            if(global_col + i < K) C[global_row * K + global_col+i] = ((float*)&res)[i];\n",
    "                        }\n",
    "                     }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "extern \"C\" void solve(const float* d_A, const float* d_B, float* d_C, int M, int N, int K) {\n",
    "    dim3 threadsPerBlock(TS / WPT, TS / WPT); \n",
    "    dim3 numBlocks((K + TS - 1) / TS, (M + TS - 1) / TS);\n",
    "    matrix_multiplication_double_buffer<<<numBlocks, threadsPerBlock>>>(d_A, d_B, d_C, M, N, K);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c1f318",
   "metadata": {},
   "source": [
    "### v4_1 scale\n",
    "\n",
    "线程块获取的数据减少"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7bf83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile v4_1_pre_vb.cu\n",
    "// 第7个版本\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "// ==========================================\n",
    "// T4 优化参数 + 双重缓冲策略\n",
    "// ==========================================\n",
    "const int BM = 128;\n",
    "const int BN = 128;\n",
    "const int BK = 8;\n",
    "const int TM = 8;\n",
    "const int TN = 8;\n",
    "\n",
    "__global__ __launch_bounds__(256)\n",
    "void sgemm_double_buffer_t4(\n",
    "    const float* __restrict__ A, \n",
    "    const float* __restrict__ B, \n",
    "    float* __restrict__ C, \n",
    "    int M, int N, int K) \n",
    "{\n",
    "    int tid = threadIdx.x;\n",
    "    int ty = tid / 16;\n",
    "    int tx = tid % 16;\n",
    "    int by = blockIdx.y;\n",
    "    int bx = blockIdx.x;\n",
    "    \n",
    "    // =========================================================\n",
    "    // 关键改变 1: 双重缓冲 Shared Memory\n",
    "    // 使用 [2] 个 buffer，write_stage 用于写，read_stage 用于算\n",
    "    // =========================================================\n",
    "    __shared__ float As[2][BK][BM]; \n",
    "    __shared__ float Bs[2][BK][BN];\n",
    "\n",
    "    float accum[TM][TN] = {0.0f};\n",
    "    \n",
    "    // 寄存器缓存，用于计算\n",
    "    float rag[TM];\n",
    "    float rbg[TN];\n",
    "\n",
    "    // 寄存器缓存，用于 Global Memory 预取 (Prefetch)\n",
    "    // 每个线程搬运 1 个 float4 的 A 和 1 个 float4 的 B\n",
    "    float4 load_a_reg; \n",
    "    float4 load_b_reg;\n",
    "\n",
    "    const float* A_ptr = A + by * BM * K;\n",
    "    const float* B_ptr = B + bx * BN;\n",
    "\n",
    "    // 加载索引计算\n",
    "    int load_a_row = tid / 2; \n",
    "    int load_a_col = (tid % 2) * 4;\n",
    "    int load_b_row = tid / 32;\n",
    "    int load_b_col = (tid % 32) * 4;\n",
    "\n",
    "    // =========================================================\n",
    "    // Prologue (序幕): 加载第一个 Tile 到 Buffer 0\n",
    "    // =========================================================\n",
    "    {\n",
    "        int k_start = 0;\n",
    "        // Load A\n",
    "        if (by * BM + load_a_row < M && k_start + load_a_col < K) {\n",
    "            load_a_reg = reinterpret_cast<const float4*>(&A_ptr[load_a_row * K + k_start + load_a_col])[0];\n",
    "        } else {\n",
    "            load_a_reg = {0.0f, 0.0f, 0.0f, 0.0f};\n",
    "        }\n",
    "        // Load B\n",
    "        if (k_start + load_b_row < K && bx * BN + load_b_col < N) {\n",
    "            load_b_reg = reinterpret_cast<const float4*>(&B_ptr[(k_start + load_b_row) * N + load_b_col])[0];\n",
    "        } else {\n",
    "            load_b_reg = {0.0f, 0.0f, 0.0f, 0.0f};\n",
    "        }\n",
    "\n",
    "        // 写入 SMEM Buffer 0\n",
    "        // A 转置写入\n",
    "        As[0][load_a_col + 0][load_a_row] = load_a_reg.x;\n",
    "        As[0][load_a_col + 1][load_a_row] = load_a_reg.y;\n",
    "        As[0][load_a_col + 2][load_a_row] = load_a_reg.z;\n",
    "        As[0][load_a_col + 3][load_a_row] = load_a_reg.w;\n",
    "        \n",
    "        // B 直接写入\n",
    "        reinterpret_cast<float4*>(&Bs[0][load_b_row][load_b_col])[0] = load_b_reg;\n",
    "    }\n",
    "\n",
    "    __syncthreads();\n",
    "\n",
    "    // =========================================================\n",
    "    // Main Loop\n",
    "    // =========================================================\n",
    "    int write_stage_idx = 1; // 下一轮写入的位置\n",
    "    int read_stage_idx = 0;  // 当前计算读取的位置\n",
    "\n",
    "    // 注意：循环从 k=0 开始算，但在 k 时我们要预加载 k+BK 的数据\n",
    "    for (int k = 0; k < K; k += BK) {\n",
    "        \n",
    "        // -----------------------------------------------------\n",
    "        // 1. Prefetch Next Tile to Registers (Global -> Register)\n",
    "        // 这里的关键是：当我们发起 Global Load 指令后，GPU 不会阻塞，\n",
    "        // 而是会继续向下执行计算指令 (Math)，从而隐藏内存延迟。\n",
    "        // -----------------------------------------------------\n",
    "        int next_k = k + BK;\n",
    "        if (next_k < K) {\n",
    "            // Load A to Reg\n",
    "            if (by * BM + load_a_row < M && next_k + load_a_col < K) {\n",
    "                load_a_reg = reinterpret_cast<const float4*>(&A_ptr[load_a_row * K + next_k + load_a_col])[0];\n",
    "            } else {\n",
    "                load_a_reg = {0.0f, 0.0f, 0.0f, 0.0f};\n",
    "            }\n",
    "            // Load B to Reg\n",
    "            if (next_k + load_b_row < K && bx * BN + load_b_col < N) {\n",
    "                load_b_reg = reinterpret_cast<const float4*>(&B_ptr[(next_k + load_b_row) * N + load_b_col])[0];\n",
    "            } else {\n",
    "                load_b_reg = {0.0f, 0.0f, 0.0f, 0.0f};\n",
    "            }\n",
    "        }\n",
    "\n",
    "        // -----------------------------------------------------\n",
    "        // 2. Compute Current Tile (Register <-> SMEM)\n",
    "        // 使用 read_stage_idx\n",
    "        // -----------------------------------------------------\n",
    "        #pragma unroll\n",
    "        for (int i = 0; i < BK; ++i) {\n",
    "            // Load A from SMEM to Reg\n",
    "            float4 tmpA0 = reinterpret_cast<const float4*>(&As[read_stage_idx][i][ty * TM])[0];\n",
    "            float4 tmpA1 = reinterpret_cast<const float4*>(&As[read_stage_idx][i][ty * TM + 4])[0];\n",
    "            rag[0] = tmpA0.x; rag[1] = tmpA0.y; rag[2] = tmpA0.z; rag[3] = tmpA0.w;\n",
    "            rag[4] = tmpA1.x; rag[5] = tmpA1.y; rag[6] = tmpA1.z; rag[7] = tmpA1.w;\n",
    "\n",
    "            // Load B from SMEM to Reg\n",
    "            float4 tmpB0 = reinterpret_cast<const float4*>(&Bs[read_stage_idx][i][tx * TN])[0];\n",
    "            float4 tmpB1 = reinterpret_cast<const float4*>(&Bs[read_stage_idx][i][tx * TN + 4])[0];\n",
    "            rbg[0] = tmpB0.x; rbg[1] = tmpB0.y; rbg[2] = tmpB0.z; rbg[3] = tmpB0.w;\n",
    "            rbg[4] = tmpB1.x; rbg[5] = tmpB1.y; rbg[6] = tmpB1.z; rbg[7] = tmpB1.w;\n",
    "\n",
    "            // Compute\n",
    "            #pragma unroll\n",
    "            for (int r = 0; r < TM; ++r) {\n",
    "                #pragma unroll\n",
    "                for (int c = 0; c < TN; ++c) {\n",
    "                    accum[r][c] += rag[r] * rbg[c];\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        // -----------------------------------------------------\n",
    "        // 3. Store Prefetched Data to SMEM (Register -> SMEM)\n",
    "        // 此时计算已经完成，我们等待所有线程都算完了当前块\n",
    "        // -----------------------------------------------------\n",
    "        __syncthreads(); // 确保 read_stage 的数据大家都不用了\n",
    "\n",
    "        if (next_k < K) {\n",
    "            // 将寄存器里的下一块数据写入 write_stage 的 SMEM\n",
    "            // A Transposed\n",
    "            As[write_stage_idx][load_a_col + 0][load_a_row] = load_a_reg.x;\n",
    "            As[write_stage_idx][load_a_col + 1][load_a_row] = load_a_reg.y;\n",
    "            As[write_stage_idx][load_a_col + 2][load_a_row] = load_a_reg.z;\n",
    "            As[write_stage_idx][load_a_col + 3][load_a_row] = load_a_reg.w;\n",
    "\n",
    "            // B Direct\n",
    "            reinterpret_cast<float4*>(&Bs[write_stage_idx][load_b_row][load_b_col])[0] = load_b_reg;\n",
    "        }\n",
    "\n",
    "        // 翻转 buffer 索引\n",
    "        // write_stage: 1 -> 0 -> 1\n",
    "        // read_stage:  0 -> 1 -> 0\n",
    "        write_stage_idx ^= 1;\n",
    "        read_stage_idx ^= 1;\n",
    "\n",
    "        __syncthreads(); // 确保 write_stage 的数据已经写好，可以作为下一轮的 read_stage\n",
    "    }\n",
    "\n",
    "    // =========================================================\n",
    "    // Write Back\n",
    "    // =========================================================\n",
    "    int global_row_start = by * BM + ty * TM;\n",
    "    int global_col_start = bx * BN + tx * TN;\n",
    "\n",
    "    #pragma unroll\n",
    "    for (int r = 0; r < TM; ++r) {\n",
    "        int global_r = global_row_start + r;\n",
    "        if (global_r < M) {\n",
    "            int global_c = global_col_start;\n",
    "            if (global_c + 7 < N) {\n",
    "                float4 tmp0, tmp1;\n",
    "                tmp0.x = accum[r][0]; tmp0.y = accum[r][1]; tmp0.z = accum[r][2]; tmp0.w = accum[r][3];\n",
    "                tmp1.x = accum[r][4]; tmp1.y = accum[r][5]; tmp1.z = accum[r][6]; tmp1.w = accum[r][7];\n",
    "                reinterpret_cast<float4*>(&C[global_r * N + global_c])[0] = tmp0;\n",
    "                reinterpret_cast<float4*>(&C[global_r * N + global_c + 4])[0] = tmp1;\n",
    "            } else {\n",
    "                for (int c = 0; c < TN; ++c) {\n",
    "                    if (global_c + c < N) {\n",
    "                        C[global_r * N + global_c + c] = accum[r][c];\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "// 宿主调用\n",
    "extern \"C\" void solve(const float* A, const float* B, float* C, int M, int N, int K) {\n",
    "    dim3 block(256);\n",
    "    dim3 grid((N + BN - 1) / BN, (M + BM - 1) / BM);\n",
    "    sgemm_double_buffer_t4<<<grid, block>>>(A, B, C, M, N, K);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb365484",
   "metadata": {},
   "source": [
    "### v5 tensor core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8f98b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile v5_tc.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <cuda_fp16.h>\n",
    "#include <mma.h>\n",
    "\n",
    "using namespace nvcuda;\n",
    "\n",
    "// --- 配置参数 ---\n",
    "// 整个 Block 处理的矩阵大小 (M, N)\n",
    "#define BM 128\n",
    "#define BN 128\n",
    "// K 维度的分块大小 (每次循环加载的深度)\n",
    "// T4 Tensor Core 基础粒度是 16，这里取 32 以平衡 Shared Memory 大小和加载效率\n",
    "#define BK 32 \n",
    "\n",
    "// 每个 Warp 处理的区域大小\n",
    "#define WM 64\n",
    "#define WN 32\n",
    "\n",
    "// Warp 的布局: 256 线程 = 8 Warps\n",
    "// 我们将 8 个 Warps 排列成 2 行 4 列 (2 * 64 = 128, 4 * 32 = 128)\n",
    "#define WARPS_M 2\n",
    "#define WARPS_N 4\n",
    "\n",
    "// Shared Memory Padding (避免 Bank Conflict)\n",
    "// half 类型占 2 字节，32个 half 是 64 字节。+8 偏移量可以错开 Bank。\n",
    "#define PAD 8\n",
    "\n",
    "__global__ void matrix_multiplication_tensor_core(\n",
    "    const float* __restrict__ A, \n",
    "    const float* __restrict__ B, \n",
    "    float* __restrict__ C, \n",
    "    int M, int N, int K) \n",
    "{\n",
    "    // --- 1. 声明 Tensor Core 片段 ---\n",
    "    // WMMA 形状: 16x16x16\n",
    "    // Frag_A matrix_a, Frag_B matrix_b, Accumulator accumulator\n",
    "    // 输入必须是 half (fp16)，累加器是 float (fp32)\n",
    "    wmma::fragment<wmma::matrix_a, 16, 16, 16, half, wmma::row_major> a_frag[WM/16][BK/16];\n",
    "    wmma::fragment<wmma::matrix_b, 16, 16, 16, half, wmma::row_major> b_frag[BK/16][WN/16];\n",
    "    wmma::fragment<wmma::accumulator, 16, 16, 16, float> c_frag[WM/16][WN/16];\n",
    "\n",
    "    // 初始化累加器为 0\n",
    "    for (int i = 0; i < WM/16; i++) {\n",
    "        for (int j = 0; j < WN/16; j++) {\n",
    "            wmma::fill_fragment(c_frag[i][j], 0.0f);\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // --- 2. 声明 Shared Memory ---\n",
    "    // A: BM x BK, B: BK x BN\n",
    "    // 使用 half 存储以供 Tensor Core 使用\n",
    "    __shared__ half As[BM][BK + PAD];\n",
    "    __shared__ half Bs[BK][BN + PAD];\n",
    "\n",
    "    // --- 3. 线程索引计算 ---\n",
    "    int tx = threadIdx.x;\n",
    "    int bx = blockIdx.x;\n",
    "    int by = blockIdx.y;\n",
    "\n",
    "    int warpId = tx / 32;\n",
    "    int laneId = tx % 32;\n",
    "\n",
    "    // 当前 Warp 在 Block 内负责的区域索引 (grid of warps)\n",
    "    int warp_row = warpId / WARPS_N; // 0 or 1\n",
    "    int warp_col = warpId % WARPS_N; // 0, 1, 2, 3\n",
    "\n",
    "    // --- 4. 主循环 (K 维度分块) ---\n",
    "    // 每次推进 BK 步\n",
    "    for (int k_step = 0; k_step < K; k_step += BK) {\n",
    "        \n",
    "        // === 加载数据 Global -> Shared 并转换 Float -> Half ===\n",
    "        // 每个线程负责加载一部分 A 和 B\n",
    "        // Block 大小 256 线程。\n",
    "        // A Tile: 128 * 32 = 4096 元素。 4096 / 256 = 16 元素/线程 (4个 float4)\n",
    "        // B Tile: 32 * 128 = 4096 元素。 16 元素/线程\n",
    "        \n",
    "        // 加载 A (BM x BK)\n",
    "        // 映射线程到 As 的坐标\n",
    "        // 我们视 As 为 (128, 32)。线程线性展开。\n",
    "        // 每个线程加载 4 个 float4 (16 floats)\n",
    "        int tid = tx;\n",
    "        int load_a_row = tid / (BK / 4); // BK=32, BK/4=8. row = tid / 8 (0~31) ?? \n",
    "        // 256 threads loading 128 rows is tricky with simple divide.\n",
    "        // Let's use a stride loop for safety and flexibility\n",
    "        \n",
    "        // 加载 A: 每个人加载 4 个 float, 循环 4 次 -> 16 floats? No.\n",
    "        // As size: 128*32 = 4096. Threads: 256. Elements per thread: 16.\n",
    "        // 使用 float4 加载，每个线程需要执行 4 次 float4 加载\n",
    "        \n",
    "        #pragma unroll\n",
    "        for (int i = 0; i < 4; ++i) {\n",
    "            // 我们把 128x32 看作线性的一维数组进行索引，然后映射回二维\n",
    "            // 总共 128 行，每行 32 元素。\n",
    "            // 每次 float4 加载 4 个元素。总共需要 128 * (32/4) = 1024 次 float4 加载。\n",
    "            // 256 线程，每个线程负责 4 次。\n",
    "            \n",
    "            int linear_idx = i * 256 + tid; // 0 ~ 1023\n",
    "            \n",
    "            // 转换 linear_idx 到 (row, col_chunk)\n",
    "            // 一行有 32/4 = 8 个 float4 块\n",
    "            int row = linear_idx / 8;\n",
    "            int col = (linear_idx % 8) * 4;\n",
    "\n",
    "            // 边界检查\n",
    "            if (row < BM && col < BK) {\n",
    "                int global_row = by * BM + row;\n",
    "                int global_col = k_step + col;\n",
    "                \n",
    "                float4 tmp = make_float4(0.0f, 0.0f, 0.0f, 0.0f);\n",
    "                if (global_row < M && global_col < K) {\n",
    "                    // 注意：这里假设 K 是 4 的倍数，否则边缘需要特殊处理\n",
    "                    // 为简化代码，这里使用 reinterpret_cast\n",
    "                    tmp = *reinterpret_cast<const float4*>(&A[global_row * K + global_col]);\n",
    "                }\n",
    "                \n",
    "                // 转换 float4 -> half (手动逐个转换或使用 intrinsics)\n",
    "                As[row][col + 0] = __float2half(tmp.x);\n",
    "                As[row][col + 1] = __float2half(tmp.y);\n",
    "                As[row][col + 2] = __float2half(tmp.z);\n",
    "                As[row][col + 3] = __float2half(tmp.w);\n",
    "            }\n",
    "        }\n",
    "\n",
    "        // 加载 B (BK x BN) -> (32 x 128)\n",
    "        // 同样 4096 元素，每线程 16 元素\n",
    "        #pragma unroll\n",
    "        for (int i = 0; i < 4; ++i) {\n",
    "            int linear_idx = i * 256 + tid;\n",
    "            // 一行有 128/4 = 32 个 float4 块\n",
    "            int row = linear_idx / 32;\n",
    "            int col = (linear_idx % 32) * 4;\n",
    "\n",
    "            if (row < BK && col < BN) {\n",
    "                int global_row = k_step + row;\n",
    "                int global_col = bx * BN + col;\n",
    "\n",
    "                float4 tmp = make_float4(0.0f, 0.0f, 0.0f, 0.0f);\n",
    "                if (global_row < K && global_col < N) { // 注意 B 是 N x K 还是 K x N? 题目中 B 是 [K][N] 布局 (row-major)\n",
    "                    // 题目原代码 B_ptr 移动逻辑: B + load_b_row * K + col. \n",
    "                    // 似乎题目原代码 B 是 [N][K] 但被视作 [N_dim][K_dim] 进行 GEMM? \n",
    "                    // 标准 GEMM: C = A(MxK) * B(KxN)。B 假如是 RowMajor，则 B[row][col] = B[k][n]。\n",
    "                    // 让我们按标准 RowMajor B(K, N) 处理。\n",
    "                    tmp = *reinterpret_cast<const float4*>(&B[global_row * N + global_col]);\n",
    "                }\n",
    "\n",
    "                Bs[row][col + 0] = __float2half(tmp.x);\n",
    "                Bs[row][col + 1] = __float2half(tmp.y);\n",
    "                Bs[row][col + 2] = __float2half(tmp.z);\n",
    "                Bs[row][col + 3] = __float2half(tmp.w);\n",
    "            }\n",
    "        }\n",
    "\n",
    "        __syncthreads();\n",
    "\n",
    "        // === Tensor Core 计算 ===\n",
    "        \n",
    "        // 每个 Warp 计算输出矩阵 C 的一块 (WM x WN) = (64 x 32)\n",
    "        // 需要在 K 维度上进一步切分 (每次切 16)\n",
    "        \n",
    "        // 内部 K 循环 (BK / 16 = 2 次)\n",
    "        for (int ki = 0; ki < BK / 16; ++ki) {\n",
    "            \n",
    "            // 1. 加载 A 的片段 (Warp 负责的行)\n",
    "            // Warp 负责的行偏移: warp_row * WM\n",
    "            // A fragment 布局: 64行，按 16行一块加载 -> 4块\n",
    "            #pragma unroll\n",
    "            for (int i = 0; i < WM / 16; ++i) {\n",
    "                // Shared Memory 地址: As[base_row + i*16][base_k]\n",
    "                int row_idx = warp_row * WM + i * 16;\n",
    "                int col_idx = ki * 16;\n",
    "                // wmma::load_matrix_sync(dst, src_ptr, stride_in_elements)\n",
    "                // Stride 是 Shared Memory 的列宽 (BK + PAD)\n",
    "                wmma::load_matrix_sync(a_frag[i][ki], &As[row_idx][col_idx], BK + PAD);\n",
    "            }\n",
    "\n",
    "            // 2. 加载 B 的片段 (Warp 负责的列)\n",
    "            // Warp 负责的列偏移: warp_col * WN\n",
    "            // B fragment 布局: 32列，按 16列一块加载 -> 2块\n",
    "            #pragma unroll\n",
    "            for (int j = 0; j < WN / 16; ++j) {\n",
    "                int row_idx = ki * 16;\n",
    "                int col_idx = warp_col * WN + j * 16;\n",
    "                wmma::load_matrix_sync(b_frag[ki][j], &Bs[row_idx][col_idx], BN + PAD);\n",
    "            }\n",
    "\n",
    "            // 3. 矩阵乘法 (Outer Product)\n",
    "            // Accumulator += A_frag * B_frag\n",
    "            #pragma unroll\n",
    "            for (int i = 0; i < WM / 16; ++i) {\n",
    "                #pragma unroll\n",
    "                for (int j = 0; j < WN / 16; ++j) {\n",
    "                    // 对应位置相乘\n",
    "                    wmma::mma_sync(c_frag[i][j], a_frag[i][ki], b_frag[ki][j], c_frag[i][j]);\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        __syncthreads();\n",
    "    }\n",
    "\n",
    "    // --- 5. 写回结果 ---\n",
    "    // Tensor Core 的结果现在在寄存器 (c_frag) 中。\n",
    "    // 需要先存回 Shared Memory (或者直接用 store_matrix_sync 存到 Global, 但通常这里需要处理 Layout)\n",
    "    // 为简单起见，利用 Shared Memory 做中转，按 float 写回 Global\n",
    "    \n",
    "    // 这里我们复用 As 或 Bs 的空间，或者直接计算 Global 指针写回。\n",
    "    // WMMA store 要求 stride。\n",
    "    // 让我们直接计算 Global Memory 地址并 Store (需要仔细处理 Stride)\n",
    "    // C 是 Row Major (M x N)。Stride = N。\n",
    "    \n",
    "    // 计算当前 Warp 在 Global C 中的起始位置\n",
    "    int c_base_row = by * BM + warp_row * WM;\n",
    "    int c_base_col = bx * BN + warp_col * WN;\n",
    "\n",
    "    #pragma unroll\n",
    "    for (int i = 0; i < WM / 16; ++i) {\n",
    "        #pragma unroll\n",
    "        for (int j = 0; j < WN / 16; ++j) {\n",
    "            int row = c_base_row + i * 16;\n",
    "            int col = c_base_col + j * 16;\n",
    "            \n",
    "            // 边界检查 (以 16x16 块为单位)\n",
    "            if (row < M && col < N) {\n",
    "                // 注意：store_matrix_sync 直接写回 Global Memory\n",
    "                // 如果 C 的内存没有对齐到 16 bytes 或者 N 不是 8/16 的倍数，可能需要注意。\n",
    "                // 这里的 layout 是 mem_row_major\n",
    "                wmma::store_matrix_sync(&C[row * N + col], c_frag[i][j], N, wmma::mem_row_major);\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "extern \"C\" void solve(const float* d_A, const float* d_B, float* d_C, int M, int N, int K) {\n",
    "    // Block Size: 256 threads (8 warps)\n",
    "    dim3 threadsPerBlock(256);\n",
    "    \n",
    "    // Grid Size\n",
    "    dim3 numBlocks((N + BN - 1) / BN, (M + BM - 1) / BM);\n",
    "    \n",
    "    matrix_multiplication_tensor_core<<<numBlocks, threadsPerBlock>>>(d_A, d_B, d_C, M, N, K);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff914cf5",
   "metadata": {},
   "source": [
    "## 编译 cu 文件为共享库 so 文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2f9c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "for v in os.listdir(os.path.abspath('.')):\n",
    "    prefix, end = os.path.splitext(v)\n",
    "    if end == '.cu':\n",
    "        subprocess.run(f\"nvcc -arch=sm_70 -shared -o {prefix}.so {prefix}.cu -Xcompiler -fPIC\", shell=True)\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e59852",
   "metadata": {},
   "source": [
    "## 进行性能测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b45a135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import ctypes\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# --- 配置参数 ---\n",
    "SIZES = [128, 512, 1024, 2048, 4096] # 测试的矩阵大小\n",
    "N_WARMUP = 5   # 预热次数\n",
    "N_ITERS = 40   # 计时的平均运行次数\n",
    "ATOL = 1    # 结果验证的容差\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# --- 辅助函数：自然排序 (v1, v2, v10...) ---\n",
    "def natural_sort_key(s):\n",
    "    return [int(text) if text.isdigit() else text.lower()\n",
    "            for text in re.split('v([0-9_]+).*', s)]\n",
    "\n",
    "# --- 核心：CUDA 库包装器 ---\n",
    "class CUDALibWrapper:\n",
    "    def __init__(self, lib_path):\n",
    "        self.lib_path = lib_path\n",
    "        self.name = os.path.splitext(os.path.basename(lib_path))[0]\n",
    "        self.lib = ctypes.CDLL(lib_path)\n",
    "        # 配置 solve 函数参数: A_ptr, B_ptr, C_ptr, M, N, K\n",
    "        self.lib.solve.argtypes = [\n",
    "            ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p,\n",
    "            ctypes.c_int, ctypes.c_int, ctypes.c_int\n",
    "        ]\n",
    "\n",
    "    def __call__(self, a_tensor, b_tensor, c_tensor):\n",
    "        M, N = a_tensor.shape\n",
    "        _, K = b_tensor.shape\n",
    "        self.lib.solve(\n",
    "            ctypes.c_void_p(a_tensor.data_ptr()),\n",
    "            ctypes.c_void_p(b_tensor.data_ptr()),\n",
    "            ctypes.c_void_p(c_tensor.data_ptr()),\n",
    "            ctypes.c_int(M), ctypes.c_int(N), ctypes.c_int(K)\n",
    "        )\n",
    "\n",
    "# --- 核心：基准测试函数 ---\n",
    "def benchmark_kernel(func, args, n_warmup, n_iters):\n",
    "    # 预热\n",
    "    for _ in range(n_warmup):\n",
    "        func(*args)\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # 计时\n",
    "    start_event = torch.cuda.Event(enable_timing=True)\n",
    "    end_event = torch.cuda.Event(enable_timing=True)\n",
    "    \n",
    "    start_event.record()\n",
    "    for _ in range(n_iters):\n",
    "        func(*args)\n",
    "    end_event.record()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # 返回平均耗时 (毫秒)\n",
    "    return start_event.elapsed_time(end_event) / n_iters\n",
    "\n",
    "# --- 主程序 ---\n",
    "def main():\n",
    "    # 1. 扫描并加载所有 .so 文件\n",
    "    so_files = [f for f in os.listdir('.') if f.endswith('.so')]\n",
    "    so_files.sort(key=natural_sort_key)\n",
    "    \n",
    "    kernels = []\n",
    "    for f in so_files:\n",
    "        try:\n",
    "            kernels.append(CUDALibWrapper(f\"./{f}\"))\n",
    "            print(f\"已加载内核: {f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"加载 {f} 失败: {e}\")\n",
    "\n",
    "    # 存储结果: { 'MethodName': [perf_size1, perf_size2, ...] }\n",
    "    results = {k.name: [] for k in kernels}\n",
    "    results['PyTorch'] = []\n",
    "    \n",
    "    # 2. 循环测试不同大小\n",
    "    for size in SIZES:\n",
    "        M, N, K = size, size, size\n",
    "        print(f\"\\n====== 正在测试尺寸: [{M}x{N}] * [{N}x{K}] ======\")\n",
    "        \n",
    "        # 准备数据\n",
    "        device = torch.device(\"cuda\")\n",
    "        A = torch.randn(M, N, device=device, dtype=torch.float32)\n",
    "        B = torch.randn(N, K, device=device, dtype=torch.float32)\n",
    "        C_ref = torch.matmul(A, B) # PyTorch 结果作为基准\n",
    "        \n",
    "        # 计算 FLOPs (2 * M * N * K)\n",
    "        flops = 2 * M * N * K\n",
    "        \n",
    "        # --- 测试 PyTorch ---\n",
    "        def run_torch(): torch.matmul(A, B)\n",
    "        avg_ms = benchmark_kernel(run_torch, (), N_WARMUP, N_ITERS)\n",
    "        tflops = (flops / (avg_ms / 1000)) / 1e12\n",
    "        results['PyTorch'].append(tflops)\n",
    "        print(f\"PyTorch \\t: {avg_ms:.3f} ms | {tflops:.3f} TFLOPS\")\n",
    "\n",
    "        # --- 测试自定义内核 ---\n",
    "        for kernel in kernels:\n",
    "            C_custom = torch.zeros((M, K), device=device, dtype=torch.float32)\n",
    "            \n",
    "            # 正确性检查\n",
    "            kernel(A, B, C_custom)\n",
    "            torch.cuda.synchronize()\n",
    "            \n",
    "            if not torch.allclose(C_custom, C_ref, atol=ATOL):\n",
    "                print(f\"{kernel.name} \\t: ❌ 结果错误 (Max Diff: {(C_custom - C_ref).abs().max().item():.4f})\")\n",
    "                results[kernel.name].append(0.0) # 错误记为 0 分\n",
    "                continue\n",
    "            \n",
    "            # 性能测试\n",
    "            def run_custom(): kernel(A, B, C_custom)\n",
    "            avg_ms = benchmark_kernel(run_custom, (), N_WARMUP, N_ITERS)\n",
    "            tflops = (flops / (avg_ms / 1000)) / 1e12\n",
    "            results[kernel.name].append(tflops)\n",
    "            print(f\"{kernel.name} \\t: {avg_ms:.3f} ms | {tflops:.3f} TFLOPS ✅\")\n",
    "\n",
    "    # 3. 绘制图表\n",
    "    plot_results_seaborn(SIZES, results)\n",
    "\n",
    "def plot_results_seaborn(sizes, results):\n",
    "    data = []\n",
    "    for name, perfs in results.items():\n",
    "        for s, p in zip(sizes, perfs):\n",
    "            data.append({\n",
    "                'Matrix Size': s,\n",
    "                'TFLOPS': p,\n",
    "                'Method': name\n",
    "            })\n",
    "    df = pd.DataFrame(data)\n",
    "    sns.set_theme(style=\"ticks\", context=\"talk\", font_scale=1.0)\n",
    "    fig, ax = plt.subplots(figsize=(14, 9))\n",
    "    methods = df['Method'].unique()\n",
    "    n_methods = len(methods)\n",
    "    palette = sns.color_palette(\"husl\", n_methods)\n",
    "    color_dict = dict(zip(methods, palette))\n",
    "    dash_dict = {m: (1, 0) for m in methods} # 默认实线\n",
    "    size_dict = {m: 2.0 for m in methods}    # 默认宽度\n",
    "\n",
    "    if 'PyTorch' in color_dict:\n",
    "        color_dict['PyTorch'] = '#333333'\n",
    "        dash_dict['PyTorch'] = (4, 2)\n",
    "        size_dict['PyTorch'] = 4.0   \n",
    "\n",
    "    sns.lineplot(\n",
    "        data=df,\n",
    "        x='Matrix Size',\n",
    "        y='TFLOPS',\n",
    "        hue='Method',\n",
    "        style='Method',\n",
    "        size='Method',\n",
    "        palette=color_dict,\n",
    "        dashes=dash_dict,\n",
    "        sizes=size_dict,\n",
    "        markers=True, \n",
    "        markersize=8,\n",
    "        ax=ax,\n",
    "        legend=False\n",
    "    )\n",
    "\n",
    "    last_points = []\n",
    "    for name in methods:\n",
    "        subset = df[df['Method'] == name]\n",
    "        last_row = subset.iloc[-1]\n",
    "        last_points.append({\n",
    "            'y': last_row['TFLOPS'],\n",
    "            'x': last_row['Matrix Size'],\n",
    "            'label': name,\n",
    "            'color': color_dict[name]\n",
    "        })\n",
    "    last_points.sort(key=lambda x: x['y'])\n",
    "    \n",
    "    all_y = df['TFLOPS'].values\n",
    "    y_span = all_y.max() - all_y.min()\n",
    "    min_dist = y_span * 0.04 \n",
    "    last_text_y = -float('inf')\n",
    "    x_max = sizes[-1]\n",
    "    ax.set_xlim(left=0, right=x_max * 1.35) \n",
    "\n",
    "    for point in last_points:\n",
    "        current_y = point['y']\n",
    "        text_y = max(current_y, last_text_y + min_dist)\n",
    "        last_text_y = text_y\n",
    "        ax.annotate(\n",
    "            text=point['label'],\n",
    "            xy=(point['x'], point['y']), \n",
    "            xytext=(x_max * 1.02, text_y),\n",
    "            color=point['color'],\n",
    "            fontweight='bold',\n",
    "            fontsize=12,\n",
    "            va='center',\n",
    "            arrowprops=dict(\n",
    "                arrowstyle=\"-\",\n",
    "                color='gray',\n",
    "                alpha=0.4,\n",
    "                lw=1,\n",
    "                shrinkB=5\n",
    "            )\n",
    "        )\n",
    "\n",
    "    ax.set_title('CUDA GEMM Performance Benchmark', pad=20, fontweight='bold')\n",
    "    ax.set_xlabel('Matrix Size (M=N=K)')\n",
    "    ax.set_ylabel('Performance (TFLOPS)')\n",
    "    ax.grid(True, which=\"major\", ls=\"--\", c='gray', alpha=0.2)\n",
    "    \n",
    "    sns.despine(trim=True, offset=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('gemm_benchmark.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
