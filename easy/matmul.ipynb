{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "922b3758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing matmul.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile matmul.cu\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "// Kernel 定义\n",
    "__global__ void matrix_multiplication_kernel(const float* A, const float* B, float* C, int M, int N, int K) {\n",
    "    int row =  blockDim.y * blockIdx.y + threadIdx.y;\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    if(col >= K || row >= M){\n",
    "        return;\n",
    "    }\n",
    "    \n",
    "    float acc = 0.0f;\n",
    "    for(int i = 0; i < N; i++){\n",
    "        acc += A[row * N + i] * B[i * K + col];\n",
    "    }\n",
    "    C[row * K + col] = acc; \n",
    "}\n",
    "\n",
    "// 宿主端 wrapper 函数\n",
    "extern \"C\" void solve(const float* A, const float* B, float* C, int M, int N, int K) {\n",
    "    dim3 threadsPerBlock(16, 16);\n",
    "    // 注意：grid 的计算需要向上取整，你的代码已经包含了这个逻辑，但建议加上括号保证运算顺序\n",
    "    dim3 blocksPerGrid((K + threadsPerBlock.x - 1) / threadsPerBlock.x,\n",
    "                       (M + threadsPerBlock.y - 1) / threadsPerBlock.y);\n",
    "    \n",
    "    matrix_multiplication_kernel<<<blocksPerGrid, threadsPerBlock>>>(A, B, C, M, N, K);\n",
    "    \n",
    "    // 检查是否有错误发生（调试用）\n",
    "    cudaError_t err = cudaGetLastError();\n",
    "    if (err != cudaSuccess) {\n",
    "        printf(\"CUDA Error: %s\\n\", cudaGetErrorString(err));\n",
    "    }\n",
    "    \n",
    "    // 等待 GPU 完成\n",
    "    cudaDeviceSynchronize();\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99606131",
   "metadata": {},
   "source": [
    "上面的实现有一个很大的问题就是，在每一个线程里面，为了计算C里面的一个位置上面的值，需要去全局内存上访问col次数据A，也需要访问col次数据B，导致数据等待时间大大增加。解决方法在于，使用线程块里面的共享内存，在一个线程块里面，先读取A和B的一部分数据到线程块里面的共享内存里面，然后进行计算，接着读取下一部分。通过分块避免所有线程去读取全局内存里面的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b98cae66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing matmul_v2.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile matmul_v2.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#define TILE_WIDTH 32 // 假设 BlockDim 为 32x32\n",
    "\n",
    "__global__ void matrix_multiplication_shared_mem(const float* __restrict__ A, const float* __restrict__ B, float* C, int M, int N, int K) {\n",
    "    // 申请共享内存\n",
    "    __shared__ float As[TILE_WIDTH][TILE_WIDTH];\n",
    "    __shared__ float Bs[TILE_WIDTH][TILE_WIDTH];\n",
    "\n",
    "    int bx = blockIdx.x;\n",
    "    int by = blockIdx.y;\n",
    "    int tx = threadIdx.x;\n",
    "    int ty = threadIdx.y;\n",
    "\n",
    "    // 计算当前线程在全局矩阵 C 中负责的坐标\n",
    "    int row = by * TILE_WIDTH + ty;\n",
    "    int col = bx * TILE_WIDTH + tx;\n",
    "\n",
    "    float acc = 0.0f;\n",
    "\n",
    "    // 循环遍历所有的 Tile (以 TILE_WIDTH 为步长遍历 N 维度)\n",
    "    for (int t = 0; t < (N + TILE_WIDTH - 1) / TILE_WIDTH; ++t) {\n",
    "\n",
    "        // 1. 协作加载 A 的 Tile 到共享内存\n",
    "        // 边界检查：防止索引越界 (Padding 0)\n",
    "        if (row < M && t * TILE_WIDTH + tx < N) {\n",
    "            As[ty][tx] = A[row * N + t * TILE_WIDTH + tx];\n",
    "        } else {\n",
    "            As[ty][tx] = 0.0f;\n",
    "        }\n",
    "\n",
    "        // 2. 协作加载 B 的 Tile 到共享内存\n",
    "        if (col < K && t * TILE_WIDTH + ty < N) {\n",
    "            // 注意这里 B 的索引：行是 t*TILE_WIDTH + ty, 列是 col\n",
    "            Bs[ty][tx] = B[(t * TILE_WIDTH + ty) * K + col];\n",
    "        } else {\n",
    "            Bs[ty][tx] = 0.0f;\n",
    "        }\n",
    "\n",
    "        // 3. 必须同步！确保所有线程都加载完了数据\n",
    "        __syncthreads();\n",
    "\n",
    "        // 4. 在共享内存上进行计算\n",
    "        for (int i = 0; i < TILE_WIDTH; ++i) {\n",
    "            acc += As[ty][i] * Bs[i][tx];\n",
    "        }\n",
    "\n",
    "        // 5. 必须同步！确保在加载下一个 Tile 之前，当前 Tile 的数据已经用完\n",
    "        __syncthreads();\n",
    "    }\n",
    "\n",
    "    // 写回结果\n",
    "    if (row < M && col < K) {\n",
    "        C[row * K + col] = acc;\n",
    "    }\n",
    "}\n",
    "\n",
    "// 宿主端 wrapper 函数\n",
    "extern \"C\" void solve(const float* A, const float* B, float* C, int M, int N, int K) {\n",
    "    dim3 threadsPerBlock(TILE_WIDTH, TILE_WIDTH);\n",
    "    // 注意：grid 的计算需要向上取整，你的代码已经包含了这个逻辑，但建议加上括号保证运算顺序\n",
    "    dim3 blocksPerGrid((K + threadsPerBlock.x - 1) / threadsPerBlock.x,\n",
    "                       (M + threadsPerBlock.y - 1) / threadsPerBlock.y);\n",
    "    \n",
    "    matrix_multiplication_shared_mem<<<blocksPerGrid, threadsPerBlock>>>(A, B, C, M, N, K);\n",
    "    \n",
    "    // 检查是否有错误发生（调试用）\n",
    "    cudaError_t err = cudaGetLastError();\n",
    "    if (err != cudaSuccess) {\n",
    "        printf(\"CUDA Error: %s\\n\", cudaGetErrorString(err));\n",
    "    }\n",
    "    \n",
    "    // 等待 GPU 完成\n",
    "    cudaDeviceSynchronize();\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4ee7d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing matmul_v3.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile matmul_v3.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#define TILE_WIDTH 32 // 假设 BlockDim 为 32x32\n",
    "\n",
    "__global__ void matrix_multiplication_shared_mem(const float *__restrict__ A, const float *__restrict__ B, float *C, int M, int N, int K)\n",
    "{\n",
    "    // 申请共享内存\n",
    "    __shared__ float As[TILE_WIDTH][TILE_WIDTH];\n",
    "    __shared__ float Bs[TILE_WIDTH][TILE_WIDTH+1];\n",
    "\n",
    "    int ty = threadIdx.y;\n",
    "    int tx = threadIdx.x;\n",
    "\n",
    "    int row = blockIdx.y * TILE_WIDTH + ty;\n",
    "    int col = blockIdx.x * TILE_WIDTH + tx;\n",
    "\n",
    "    float acc = 0.0f;\n",
    "\n",
    "    for (int i = 0; i < (N + TILE_WIDTH - 1) / TILE_WIDTH; i++)\n",
    "    {\n",
    "        // 读取数据\n",
    "        if (row < M && TILE_WIDTH * i + tx < N)\n",
    "        {\n",
    "            As[ty][tx] = A[row * N + TILE_WIDTH * i + tx];\n",
    "        }\n",
    "        else\n",
    "        {\n",
    "            As[ty][tx] = 0.0f;\n",
    "        }\n",
    "        if (col < K && TILE_WIDTH * i + ty < N)\n",
    "        {\n",
    "            Bs[ty][tx] = B[(TILE_WIDTH * i + ty) * K + col];\n",
    "        }\n",
    "        else\n",
    "        {\n",
    "            Bs[ty][tx] = 0.0f;\n",
    "        }\n",
    "        __syncthreads(); // 等待线程块里面的线程都搬运完成\n",
    "\n",
    "        for (int j = 0; j < TILE_WIDTH; j++)\n",
    "        {\n",
    "            acc += As[ty][j] * Bs[j][tx]; //访问线程块里面的共享内存时，没有合并访问，只在乎bank config\n",
    "        }\n",
    "\n",
    "        __syncthreads();\n",
    "    }\n",
    "    if (row<M && col<K)\n",
    "    {\n",
    "        C[row*K+col] = acc;\n",
    "    }\n",
    "}\n",
    "\n",
    "// 宿主端 wrapper 函数\n",
    "extern \"C\" void solve(const float *A, const float *B, float *C, int M, int N, int K)\n",
    "{\n",
    "    dim3 threadsPerBlock(TILE_WIDTH, TILE_WIDTH);\n",
    "    // 注意：grid 的计算需要向上取整，你的代码已经包含了这个逻辑，但建议加上括号保证运算顺序\n",
    "    dim3 blocksPerGrid((K + threadsPerBlock.x - 1) / threadsPerBlock.x,\n",
    "                       (M + threadsPerBlock.y - 1) / threadsPerBlock.y);\n",
    "\n",
    "    matrix_multiplication_shared_mem<<<blocksPerGrid, threadsPerBlock>>>(A, B, C, M, N, K);\n",
    "\n",
    "    // 检查是否有错误发生（调试用）\n",
    "    cudaError_t err = cudaGetLastError();\n",
    "    if (err != cudaSuccess)\n",
    "    {\n",
    "        printf(\"CUDA Error: %s\\n\", cudaGetErrorString(err));\n",
    "    }\n",
    "\n",
    "    // 等待 GPU 完成\n",
    "    cudaDeviceSynchronize();\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8790ea4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing matmul_v4.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile matmul_v4.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// 宏定义块大小\n",
    "// TS (Tile Size): 每个 Block 计算 64x64 的 C\n",
    "// WPT (Work Per Thread): 每个线程计算 4x4 的 C\n",
    "// TS_K: K 维度(你的代码里是 N 维度)的分块大小，设为 8 或 16\n",
    "#define TS 64\n",
    "#define WPT 4\n",
    "#define TS_K 16 \n",
    "\n",
    "// 优化后的 Kernel\n",
    "__global__ void matrix_multiplication_optimized(\n",
    "    const float* __restrict__ A, \n",
    "    const float* __restrict__ B, \n",
    "    float* __restrict__ C, \n",
    "    int M, int N, int K) \n",
    "{\n",
    "    // 每个 Block 处理 C 中 TS x TS (64x64) 的区域\n",
    "    // 线程块维度: dim3(TS/WPT, TS/WPT) -> (16, 16) -> 256 个线程\n",
    "    \n",
    "    // 1. 声明共享内存\n",
    "    // As: 存储 A 的切片 [TS][TS_K] -> [64][16]\n",
    "    // Bs: 存储 B 的切片 [TS_K][TS] -> [16][64]\n",
    "    __shared__ float As[TS][TS_K];\n",
    "    __shared__ float Bs[TS_K][TS];\n",
    "\n",
    "    // 2. 声明寄存器\n",
    "    // accum: 累加器，每个线程负责计算 4x4 = 16 个元素\n",
    "    float accum[WPT][WPT] = {0.0f};\n",
    "    \n",
    "    // reg_A, reg_B: 用于在内循环中缓存从 SMEM 读取的值\n",
    "    float reg_A[WPT];\n",
    "    float reg_B[WPT];\n",
    "\n",
    "    // 线程 ID 和 Block ID\n",
    "    int tx = threadIdx.x; // range 0-15\n",
    "    int ty = threadIdx.y; // range 0-15\n",
    "    int bx = blockIdx.x;\n",
    "    int by = blockIdx.y;\n",
    "\n",
    "    // 当前线程负责的 C 矩阵起始坐标 (C 的分块左上角 + 线程偏移)\n",
    "    // 每个线程覆盖 WPT(4) 个像素宽/高\n",
    "    int row_c = by * TS + ty * WPT; \n",
    "    int col_c = bx * TS + tx * WPT;\n",
    "\n",
    "    // 3. 循环遍历 N 维度 (步长 TS_K = 16)\n",
    "    for (int t = 0; t < N; t += TS_K) {\n",
    "        \n",
    "        // --- 加载数据到 Shared Memory (协作加载) ---\n",
    "        // 我们有 256 个线程。\n",
    "        // 需要加载 A 的 Tile: 64行 * 16列 = 1024 元素。每个线程加载 1024/256 = 4 个元素。\n",
    "        // 需要加载 B 的 Tile: 16行 * 64列 = 1024 元素。每个线程加载 4 个元素。\n",
    "        \n",
    "        // 加载 As (A 的子块): \n",
    "        // 这里的逻辑是将 256 个线程映射到 64x16 的区域\n",
    "        // 我们使用 float4 向量化加载来极致优化带宽\n",
    "        \n",
    "        // 计算当前线程加载 As 的位置\n",
    "        // 将 16x16 的线程块视为 256 个线性线程\n",
    "        int tid = ty * (TS / WPT) + tx; // 0 ~ 255\n",
    "        \n",
    "        // 映射到 As[64][16]: 每一行 16 个元素，如果是 float4 就是 4 个 float4\n",
    "        // 256 个线程，每个加载 1 个 float4 (4个float)，正好 1024 个 float\n",
    "        // As 的行索引\n",
    "        int load_a_row = tid / (TS_K / 4); \n",
    "        int load_a_col = (tid % (TS_K / 4)) * 4;\n",
    "        \n",
    "        // 从全局内存 A 加载到 As\n",
    "        // 全局索引: A[(by * TS + load_a_row) * N + (t + load_a_col)]\n",
    "        // 注意边界检查省略了，假设维度对其\n",
    "        if (by * TS + load_a_row < M && t + load_a_col < N) {\n",
    "             // 使用 float4 指针强转进行向量加载\n",
    "             float4 tmp = reinterpret_cast<const float4*>(&A[(by * TS + load_a_row) * N + (t + load_a_col)])[0];\n",
    "             As[load_a_row][load_a_col + 0] = tmp.x;\n",
    "             As[load_a_row][load_a_col + 1] = tmp.y;\n",
    "             As[load_a_row][load_a_col + 2] = tmp.z;\n",
    "             As[load_a_row][load_a_col + 3] = tmp.w;\n",
    "        }\n",
    "\n",
    "        // 加载 Bs (B 的子块): [16][64]\n",
    "        // 同样用 tid 映射。每行 64 个元素 = 16 个 float4。\n",
    "        // 总共 16 行。总 float4 数 = 16 * 16 = 256。正好每个线程取 1 个 float4。\n",
    "        int load_b_row = tid / (TS / 4);\n",
    "        int load_b_col = (tid % (TS / 4)) * 4;\n",
    "\n",
    "        if (t + load_b_row < N && bx * TS + load_b_col < K) {\n",
    "             float4 tmp = reinterpret_cast<const float4*>(&B[(t + load_b_row) * K + (bx * TS + load_b_col)])[0];\n",
    "             Bs[load_b_row][load_b_col + 0] = tmp.x;\n",
    "             Bs[load_b_row][load_b_col + 1] = tmp.y;\n",
    "             Bs[load_b_row][load_b_col + 2] = tmp.z;\n",
    "             Bs[load_b_row][load_b_col + 3] = tmp.w;\n",
    "        }\n",
    "\n",
    "        __syncthreads(); // 等待数据加载完成\n",
    "\n",
    "        // --- 在寄存器上进行计算 ---\n",
    "        // 遍历 Shared Memory 中的 TS_K (16) 维度\n",
    "        #pragma unroll\n",
    "        for (int k = 0; k < TS_K; ++k) {\n",
    "            \n",
    "            // 1. 将所需的 As 和 Bs 数据预加载到寄存器\n",
    "            // 每个线程计算 4x4，需要 As 的一列 4 个值，Bs 的一行 4 个值\n",
    "            for (int i = 0; i < WPT; ++i) {\n",
    "                reg_A[i] = As[ty * WPT + i][k];\n",
    "                reg_B[i] = Bs[k][tx * WPT + i];\n",
    "            }\n",
    "\n",
    "            // 2. 外积计算 (Outer Product)\n",
    "            // 计算 4x4 的结果，复用 reg_A 和 reg_B\n",
    "            for (int row = 0; row < WPT; ++row) {\n",
    "                for (int col = 0; col < WPT; ++col) {\n",
    "                    accum[row][col] += reg_A[row] * reg_B[col];\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        __syncthreads(); // 等待计算完成，准备加载下一块\n",
    "    }\n",
    "\n",
    "    // 4. 写回结果到全局内存\n",
    "    // 每个线程写回 4x4 个点\n",
    "    for (int row = 0; row < WPT; ++row) {\n",
    "        for (int col = 0; col < WPT; ++col) {\n",
    "            int global_row = row_c + row;\n",
    "            int global_col = col_c + col;\n",
    "            \n",
    "            if (global_row < M && global_col < K) {\n",
    "                C[global_row * K + global_col] = accum[row][col];\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "// Host 端调用示例\n",
    "extern \"C\" void solve(const float* d_A, const float* d_B, float* d_C, int M, int N, int K) {\n",
    "    // 线程块大小: 16x16 = 256 线程\n",
    "    dim3 threadsPerBlock(TS / WPT, TS / WPT); \n",
    "    \n",
    "    // Grid 大小: 因为每个 Block 处理 64x64，所以除以 TS(64)\n",
    "    dim3 numBlocks((K + TS - 1) / TS, (M + TS - 1) / TS);\n",
    "\n",
    "    matrix_multiplication_optimized<<<numBlocks, threadsPerBlock>>>(d_A, d_B, d_C, M, N, K);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91bf7658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing matmul_v4_1.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile matmul_v4_1.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// 宏定义块大小\n",
    "// TS (Tile Size): 每个 Block 计算 64x64 的 C\n",
    "// WPT (Work Per Thread): 每个线程计算 4x4 的 C\n",
    "// TS_K: K 维度(你的代码里是 N 维度)的分块大小，设为 8 或 16\n",
    "#define TS 64\n",
    "#define WPT 4\n",
    "#define TS_K 16 \n",
    "\n",
    "// 优化后的 Kernel\n",
    "__global__ void matrix_multiplication_optimized(\n",
    "    const float* __restrict__ A, \n",
    "    const float* __restrict__ B, \n",
    "    float* __restrict__ C, \n",
    "    int M, int N, int K) \n",
    "{\n",
    "    // 每个 Block 处理 C 中 TS x TS (64x64) 的区域\n",
    "    // 线程块维度: dim3(TS/WPT, TS/WPT) -> (16, 16) -> 256 个线程\n",
    "    \n",
    "    // 1. 声明共享内存\n",
    "    // As: 存储 A 的切片 [TS][TS_K] -> [64][16]\n",
    "    // Bs: 存储 B 的切片 [TS_K][TS] -> [16][64]\n",
    "    __shared__ float As[TS][TS_K];\n",
    "    __shared__ float Bs[TS_K][TS];\n",
    "\n",
    "    // 2. 声明寄存器\n",
    "    // accum: 累加器，每个线程负责计算 4x4 = 16 个元素\n",
    "    float accum[WPT][WPT] = {0.0f};\n",
    "    \n",
    "    // reg_A, reg_B: 用于在内循环中缓存从 SMEM 读取的值\n",
    "    float reg_A[WPT];\n",
    "    float reg_B[WPT];\n",
    "\n",
    "    // 线程 ID 和 Block ID\n",
    "    int tx = threadIdx.x; // range 0-15\n",
    "    int ty = threadIdx.y; // range 0-15\n",
    "    int bx = blockIdx.x;\n",
    "    int by = blockIdx.y;\n",
    "\n",
    "    // 当前线程负责的 C 矩阵起始坐标 (C 的分块左上角 + 线程偏移)\n",
    "    // 每个线程覆盖 WPT(4) 个像素宽/高\n",
    "    int row_c = by * TS + ty * WPT; \n",
    "    int col_c = bx * TS + tx * WPT;\n",
    "\n",
    "    // 3. 循环遍历 N 维度 (步长 TS_K = 16)\n",
    "    for (int t = 0; t < N; t += TS_K) {\n",
    "        \n",
    "        // --- 加载数据到 Shared Memory (协作加载) ---\n",
    "        // 我们有 256 个线程。\n",
    "        // 需要加载 A 的 Tile: 64行 * 16列 = 1024 元素。每个线程加载 1024/256 = 4 个元素。\n",
    "        // 需要加载 B 的 Tile: 16行 * 64列 = 1024 元素。每个线程加载 4 个元素。\n",
    "        \n",
    "        // 加载 As (A 的子块): \n",
    "        // 这里的逻辑是将 256 个线程映射到 64x16 的区域\n",
    "        // 我们使用 float4 向量化加载来极致优化带宽\n",
    "        \n",
    "        // 计算当前线程加载 As 的位置\n",
    "        // 将 16x16 的线程块视为 256 个线性线程\n",
    "        int tid = ty * (TS / WPT) + tx; // 0 ~ 255\n",
    "        \n",
    "        // 映射到 As[64][16]: 每一行 16 个元素，如果是 float4 就是 4 个 float4\n",
    "        // 256 个线程，每个加载 1 个 float4 (4个float)，正好 1024 个 float\n",
    "        // As 的行索引\n",
    "        int load_a_row = tid / (TS_K / 4); \n",
    "        int load_a_col = (tid % (TS_K / 4)) * 4;\n",
    "        \n",
    "        // 从全局内存 A 加载到 As\n",
    "        // 全局索引: A[(by * TS + load_a_row) * N + (t + load_a_col)]\n",
    "        // 注意边界检查省略了，假设维度对其\n",
    "        if (by * TS + load_a_row < M && t + load_a_col < N) {\n",
    "             // 使用 float4 指针强转进行向量加载\n",
    "             float4 tmp = reinterpret_cast<const float4*>(&A[(by * TS + load_a_row) * N + (t + load_a_col)])[0];\n",
    "             As[load_a_row][load_a_col + 0] = tmp.x;\n",
    "             As[load_a_row][load_a_col + 1] = tmp.y;\n",
    "             As[load_a_row][load_a_col + 2] = tmp.z;\n",
    "             As[load_a_row][load_a_col + 3] = tmp.w;\n",
    "        }\n",
    "\n",
    "        // 加载 Bs (B 的子块): [16][64]\n",
    "        // 同样用 tid 映射。每行 64 个元素 = 16 个 float4。\n",
    "        // 总共 16 行。总 float4 数 = 16 * 16 = 256。正好每个线程取 1 个 float4。\n",
    "        int load_b_row = tid / (TS / 4);\n",
    "        int load_b_col = (tid % (TS / 4)) * 4;\n",
    "\n",
    "        if (t + load_b_row < N && bx * TS + load_b_col < K) {\n",
    "             float4 tmp = reinterpret_cast<const float4*>(&B[(t + load_b_row) * K + (bx * TS + load_b_col)])[0];\n",
    "             Bs[load_b_row][load_b_col + 0] = tmp.x;\n",
    "             Bs[load_b_row][load_b_col + 1] = tmp.y;\n",
    "             Bs[load_b_row][load_b_col + 2] = tmp.z;\n",
    "             Bs[load_b_row][load_b_col + 3] = tmp.w;\n",
    "        }\n",
    "\n",
    "        __syncthreads(); // 等待数据加载完成\n",
    "\n",
    "        // --- 在寄存器上进行计算 ---\n",
    "        // 遍历 Shared Memory 中的 TS_K (16) 维度\n",
    "        #pragma unroll\n",
    "        for (int k = 0; k < TS_K; ++k) {\n",
    "            \n",
    "            // 1. 将所需的 As 和 Bs 数据预加载到寄存器\n",
    "            // 每个线程计算 4x4，需要 As 的一列 4 个值，Bs 的一行 4 个值\n",
    "            for (int i = 0; i < WPT; ++i) {\n",
    "                reg_A[i] = As[ty * WPT + i][k];\n",
    "                reg_B[i] = Bs[k][tx * WPT + i];\n",
    "            }\n",
    "\n",
    "            // 2. 外积计算 (Outer Product)\n",
    "            // 计算 4x4 的结果，复用 reg_A 和 reg_B\n",
    "            for (int row = 0; row < WPT; ++row) {\n",
    "                for (int col = 0; col < WPT; ++col) {\n",
    "                    accum[row][col] += reg_A[row] * reg_B[col];\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        __syncthreads(); // 等待计算完成，准备加载下一块\n",
    "    }\n",
    "\n",
    "    // 4. 写回结果到全局内存\n",
    "    // 每个线程写回 4x4 个点\n",
    "    for (int row = 0; row < WPT; ++row) {\n",
    "        for (int col = 0; col < WPT; ++col) {\n",
    "            int global_row = row_c + row;\n",
    "            int global_col = col_c + col;\n",
    "            \n",
    "            if (global_row < M && global_col < K) {\n",
    "                C[global_row * K + global_col] = accum[row][col];\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "// Host 端调用示例\n",
    "extern \"C\" void solve(const float* d_A, const float* d_B, float* d_C, int M, int N, int K) {\n",
    "    // 线程块大小: 16x16 = 256 线程\n",
    "    dim3 threadsPerBlock(TS / WPT, TS / WPT); \n",
    "    \n",
    "    // Grid 大小: 因为每个 Block 处理 64x64，所以除以 TS(64)\n",
    "    dim3 numBlocks((K + TS - 1) / TS, (M + TS - 1) / TS);\n",
    "\n",
    "    matrix_multiplication_optimized<<<numBlocks, threadsPerBlock>>>(d_A, d_B, d_C, M, N, K);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67e6b92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing matmul_v5.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile matmul_v5.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// ==========================================\n",
    "// 针对 T4 (Compute Capability 7.5) 优化的参数\n",
    "// ==========================================\n",
    "// 每个 Block 计算 128x128 的 C\n",
    "const int BM = 128;\n",
    "const int BN = 128;\n",
    "// K 维度的步进，每次加载 8 列\n",
    "const int BK = 8;\n",
    "// 每个线程计算 8x8 的 C\n",
    "const int TM = 8;\n",
    "const int TN = 8;\n",
    "\n",
    "// 线程块大小: (BM/TM) * (BN/TN) = 16 * 16 = 256 线程\n",
    "// 满足 T4 最佳 Occupancy\n",
    "\n",
    "__global__ __launch_bounds__(256)\n",
    "void sgemm_optimized_t4(\n",
    "    const float* __restrict__ A, \n",
    "    const float* __restrict__ B, \n",
    "    float* __restrict__ C, \n",
    "    int M, int N, int K) \n",
    "{\n",
    "    // 线程索引\n",
    "    int tid = threadIdx.x; // 0..255\n",
    "    \n",
    "    // 逻辑坐标 (16x16)\n",
    "    int ty = tid / 16;\n",
    "    int tx = tid % 16;\n",
    "\n",
    "    // 当前 Block 负责的 C 的左上角坐标\n",
    "    int by = blockIdx.y;\n",
    "    int bx = blockIdx.x;\n",
    "    \n",
    "    // Shared Memory 声明\n",
    "    // As: 存储 A 的切片 [BK][BM]。\n",
    "    // 注意：这里我们将 A 转置存储 (Transposed)，为了后续计算时能向量化读取\n",
    "    // Padding: 为了避免 Bank Conflicts，我们在行尾 +1 或 +4 (这里不需要，因为 128 是 32 的倍数，但读取方式不同)\n",
    "    // 实际上，为了极致性能，我们让 As 的维度为 [BK][BM]，这样计算时按 As[k][row] 读取是连续的。\n",
    "    __shared__ float As[BK][BM]; \n",
    "    __shared__ float Bs[BK][BN];\n",
    "\n",
    "    // 寄存器累加器，8x8\n",
    "    float accum[TM][TN] = {0.0f};\n",
    "\n",
    "    // 用于从 SMEM 加载到寄存器的临时变量\n",
    "    float rag[TM]; // 缓存 A 的一列\n",
    "    float rbg[TN]; // 缓存 B 的一行\n",
    "\n",
    "    // 计算当前 Block 在 Global Memory 中的起始位置\n",
    "    const float* A_ptr = A + by * BM * K;\n",
    "    const float* B_ptr = B + bx * BN;\n",
    "    float* C_ptr = C + by * BM * N + bx * BN;\n",
    "\n",
    "    // ==========================================================\n",
    "    // 预计算加载 Global Memory 的索引\n",
    "    // 我们有 256 个线程。\n",
    "    // A 的 Tile 是 128(Row) x 8(Col)。总共 1024 元素。每个线程搬运 4 个 (float4)。\n",
    "    // B 的 Tile 是 8(Row) x 128(Col)。总共 1024 元素。每个线程搬运 4 个 (float4)。\n",
    "    // ==========================================================\n",
    "\n",
    "    // A 的加载索引 (为了转置存储到 SMEM)\n",
    "    // 我们按 Global A 的行优先读取，写入到 As 的 [col][row]\n",
    "    // tid 范围 0-255。\n",
    "    // 128行 * 8列 / 4(float4) = 256 个 float4 操作。正好 1 线程 1 个 float4。\n",
    "    // load_a_row: 0..127, load_a_col: 0, 4 (因为 K step 是 8)\n",
    "    int load_a_row = tid / 2; \n",
    "    int load_a_col = (tid % 2) * 4;\n",
    "\n",
    "    // B 的加载索引\n",
    "    // 8行 * 128列 / 4 = 256 个 float4。\n",
    "    int load_b_row = tid / 32;\n",
    "    int load_b_col = (tid % 32) * 4;\n",
    "\n",
    "    // 主循环：在 K 维度上推进\n",
    "    for (int k_step = 0; k_step < K; k_step += BK) {\n",
    "        \n",
    "        // --------------------------------------------------------\n",
    "        // 1. 加载数据到 Shared Memory (Vectorized Global Load)\n",
    "        // --------------------------------------------------------\n",
    "        \n",
    "        // 加载 A (M x K): 使用 float4\n",
    "        // 边界检查：假设 M, N, K 是 8/128 的倍数以获得最佳性能，这里加简单防护\n",
    "        if (by * BM + load_a_row < M && k_step + load_a_col < K) {\n",
    "            float4 tmp = reinterpret_cast<const float4*>(&A_ptr[load_a_row * K + k_step + load_a_col])[0];\n",
    "            // 关键优化：转置写入 As\n",
    "            // Global: A[row][k] -> SMEM: As[k][row]\n",
    "            // 这样在计算阶段，同一个 k 的不同 row 是连续内存\n",
    "            As[load_a_col + 0][load_a_row] = tmp.x;\n",
    "            As[load_a_col + 1][load_a_row] = tmp.y;\n",
    "            As[load_a_col + 2][load_a_row] = tmp.z;\n",
    "            As[load_a_col + 3][load_a_row] = tmp.w;\n",
    "        } else {\n",
    "            // 边界 padding (设为0不影响累加)\n",
    "             As[load_a_col + 0][load_a_row] = 0.0f;\n",
    "             As[load_a_col + 1][load_a_row] = 0.0f;\n",
    "             As[load_a_col + 2][load_a_row] = 0.0f;\n",
    "             As[load_a_col + 3][load_a_row] = 0.0f;\n",
    "        }\n",
    "\n",
    "        // 加载 B (K x N): 使用 float4\n",
    "        if (k_step + load_b_row < K && bx * BN + load_b_col < N) {\n",
    "            float4 tmp = reinterpret_cast<const float4*>(&B_ptr[(k_step + load_b_row) * N + load_b_col])[0];\n",
    "            // B 不需要转置，直接按行存\n",
    "            reinterpret_cast<float4*>(&Bs[load_b_row][load_b_col])[0] = tmp;\n",
    "        } else {\n",
    "             Bs[load_b_row][load_b_col + 0] = 0.0f;\n",
    "             Bs[load_b_row][load_b_col + 1] = 0.0f;\n",
    "             Bs[load_b_row][load_b_col + 2] = 0.0f;\n",
    "             Bs[load_b_row][load_b_col + 3] = 0.0f;\n",
    "        }\n",
    "\n",
    "        __syncthreads();\n",
    "\n",
    "        // --------------------------------------------------------\n",
    "        // 2. 计算 (Math Loop)\n",
    "        // --------------------------------------------------------\n",
    "        // 展开循环，计算 8 个 K 步骤\n",
    "        #pragma unroll\n",
    "        for (int k = 0; k < BK; ++k) {\n",
    "            // 从 SMEM 加载 A 的一列 (TM=8) 到寄存器\n",
    "            // 由于我们转置了 As，现在 As[k][row...] 是连续的！\n",
    "            // 我们可以用 float4 加载来加速！\n",
    "            // 当前线程计算的 C 的行是: ty * TM 到 ty * TM + 7\n",
    "            // 对应的 SMEM 地址是: &As[k][ty * TM]\n",
    "            float4 tmpA0 = reinterpret_cast<const float4*>(&As[k][ty * TM])[0];\n",
    "            float4 tmpA1 = reinterpret_cast<const float4*>(&As[k][ty * TM + 4])[0];\n",
    "            \n",
    "            rag[0] = tmpA0.x; rag[1] = tmpA0.y; rag[2] = tmpA0.z; rag[3] = tmpA0.w;\n",
    "            rag[4] = tmpA1.x; rag[5] = tmpA1.y; rag[6] = tmpA1.z; rag[7] = tmpA1.w;\n",
    "\n",
    "            // 从 SMEM 加载 B 的一行 (TN=8) 到寄存器\n",
    "            // 当前线程计算的 C 的列是: tx * TN 到 tx * TN + 7\n",
    "            // 对应的 SMEM 地址是: &Bs[k][tx * TN]\n",
    "            float4 tmpB0 = reinterpret_cast<const float4*>(&Bs[k][tx * TN])[0];\n",
    "            float4 tmpB1 = reinterpret_cast<const float4*>(&Bs[k][tx * TN + 4])[0];\n",
    "\n",
    "            rbg[0] = tmpB0.x; rbg[1] = tmpB0.y; rbg[2] = tmpB0.z; rbg[3] = tmpB0.w;\n",
    "            rbg[4] = tmpB1.x; rbg[5] = tmpB1.y; rbg[6] = tmpB1.z; rbg[7] = tmpB1.w;\n",
    "\n",
    "            // 外积 (Outer Product) 计算 8x8\n",
    "            // 编译器会自动优化为 FFMA 指令\n",
    "            #pragma unroll\n",
    "            for (int r = 0; r < TM; ++r) {\n",
    "                #pragma unroll\n",
    "                for (int c = 0; c < TN; ++c) {\n",
    "                    accum[r][c] += rag[r] * rbg[c];\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        __syncthreads();\n",
    "    }\n",
    "\n",
    "    // --------------------------------------------------------\n",
    "    // 3. 写回 Global Memory\n",
    "    // --------------------------------------------------------\n",
    "    // 每个线程负责 8x8 个点。\n",
    "    // 为了带宽优化，我们也应该用 float4 写回。\n",
    "    // 这稍微复杂一点，因为 thread 的 8x8 是块状的，不是完全连续的行。\n",
    "    // 每个线程有 8 行，每行 8 个元素。每行的 8 个元素是连续的。\n",
    "    // 可以用 2 个 float4 写回一行。\n",
    "\n",
    "    int global_row_start = by * BM + ty * TM;\n",
    "    int global_col_start = bx * BN + tx * TN;\n",
    "\n",
    "    #pragma unroll\n",
    "    for (int r = 0; r < TM; ++r) {\n",
    "        int global_r = global_row_start + r;\n",
    "        if (global_r < M) {\n",
    "            int global_c = global_col_start;\n",
    "            if (global_c + 7 < N) {\n",
    "                // 常见的路径：可以直接 float4 写回\n",
    "                float4 tmp0;\n",
    "                tmp0.x = accum[r][0]; tmp0.y = accum[r][1]; tmp0.z = accum[r][2]; tmp0.w = accum[r][3];\n",
    "                reinterpret_cast<float4*>(&C[global_r * N + global_c])[0] = tmp0;\n",
    "\n",
    "                float4 tmp1;\n",
    "                tmp1.x = accum[r][4]; tmp1.y = accum[r][5]; tmp1.z = accum[r][6]; tmp1.w = accum[r][7];\n",
    "                reinterpret_cast<float4*>(&C[global_r * N + global_c + 4])[0] = tmp1;\n",
    "            } else {\n",
    "                // 边界处理\n",
    "                for (int c = 0; c < TN; ++c) {\n",
    "                    if (global_c + c < N) {\n",
    "                        C[global_r * N + global_c + c] = accum[r][c];\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "// 主机端调用 Wrapper\n",
    "extern \"C\" void solve(const float* A, const float* B, float* C, int M, int N, int K) {\n",
    "    dim3 block(256);\n",
    "    dim3 grid((N + BN - 1) / BN, (M + BM - 1) / BM);\n",
    "    \n",
    "    sgemm_optimized_t4<<<grid, block>>>(A, B, C, M, N, K);\n",
    "}\n",
    "\n",
    "// 测试 main 函数 (可选)\n",
    "int main() {\n",
    "    int M = 4096;\n",
    "    int N = 4096;\n",
    "    int K = 4096;\n",
    "    size_t size = M * K * sizeof(float); // 简化测试，假设方阵\n",
    "\n",
    "    float *h_A, *h_B, *h_C, *d_A, *d_B, *d_C;\n",
    "    \n",
    "    // 分配内存... (略去分配和初始化代码以保持简洁)\n",
    "    // cudaMalloc(&d_A, ...);\n",
    "    \n",
    "    // 调用\n",
    "    // launch_sgemm_optimized(d_A, d_B, d_C, M, N, K);\n",
    "    \n",
    "    // cudaDeviceSynchronize();\n",
    "    \n",
    "    printf(\"Kernel compiled and structure ready for T4.\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e1704fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting matmul_v6.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile matmul_v6.cu\n",
    "// 第6个版本\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "// -------------------------------------------------------------------------\n",
    "// 参数定义 (针对 T4 优化)\n",
    "// -------------------------------------------------------------------------\n",
    "const int BM = 128;\n",
    "const int BN = 128;\n",
    "const int BK = 8;\n",
    "const int TM = 8;\n",
    "const int TN = 8;\n",
    "\n",
    "__global__ __launch_bounds__(256)\n",
    "void sgemm_optimized_t4_kernel(\n",
    "    const float* __restrict__ A, \n",
    "    const float* __restrict__ B, \n",
    "    float* __restrict__ C, \n",
    "    int M, int N, int K) \n",
    "{\n",
    "    int tid = threadIdx.x;\n",
    "    int ty = tid / 16;\n",
    "    int tx = tid % 16;\n",
    "    int by = blockIdx.y;\n",
    "    int bx = blockIdx.x;\n",
    "    \n",
    "    __shared__ float As[BK][BM]; \n",
    "    __shared__ float Bs[BK][BN];\n",
    "\n",
    "    float accum[TM][TN] = {0.0f};\n",
    "    float rag[TM];\n",
    "    float rbg[TN];\n",
    "\n",
    "    const float* A_ptr = A + by * BM * K;\n",
    "    const float* B_ptr = B + bx * BN;\n",
    "    // C 指针后面计算\n",
    "\n",
    "    int load_a_row = tid / 2; \n",
    "    int load_a_col = (tid % 2) * 4;\n",
    "    int load_b_row = tid / 32;\n",
    "    int load_b_col = (tid % 32) * 4;\n",
    "\n",
    "    for (int k_step = 0; k_step < K; k_step += BK) {\n",
    "        // Load A (Transposed)\n",
    "        if (by * BM + load_a_row < M && k_step + load_a_col < K) {\n",
    "            float4 tmp = reinterpret_cast<const float4*>(&A_ptr[load_a_row * K + k_step + load_a_col])[0];\n",
    "            As[load_a_col + 0][load_a_row] = tmp.x;\n",
    "            As[load_a_col + 1][load_a_row] = tmp.y;\n",
    "            As[load_a_col + 2][load_a_row] = tmp.z;\n",
    "            As[load_a_col + 3][load_a_row] = tmp.w;\n",
    "        } else {\n",
    "             As[load_a_col + 0][load_a_row] = 0.0f;\n",
    "             As[load_a_col + 1][load_a_row] = 0.0f;\n",
    "             As[load_a_col + 2][load_a_row] = 0.0f;\n",
    "             As[load_a_col + 3][load_a_row] = 0.0f;\n",
    "        }\n",
    "\n",
    "        // Load B\n",
    "        if (k_step + load_b_row < K && bx * BN + load_b_col < N) {\n",
    "            float4 tmp = reinterpret_cast<const float4*>(&B_ptr[(k_step + load_b_row) * N + load_b_col])[0];\n",
    "            reinterpret_cast<float4*>(&Bs[load_b_row][load_b_col])[0] = tmp;\n",
    "        } else {\n",
    "             Bs[load_b_row][load_b_col + 0] = 0.0f;\n",
    "             Bs[load_b_row][load_b_col + 1] = 0.0f;\n",
    "             Bs[load_b_row][load_b_col + 2] = 0.0f;\n",
    "             Bs[load_b_row][load_b_col + 3] = 0.0f;\n",
    "        }\n",
    "\n",
    "        __syncthreads();\n",
    "\n",
    "        #pragma unroll\n",
    "        for (int k = 0; k < BK; ++k) {\n",
    "            float4 tmpA0 = reinterpret_cast<const float4*>(&As[k][ty * TM])[0];\n",
    "            float4 tmpA1 = reinterpret_cast<const float4*>(&As[k][ty * TM + 4])[0];\n",
    "            rag[0] = tmpA0.x; rag[1] = tmpA0.y; rag[2] = tmpA0.z; rag[3] = tmpA0.w;\n",
    "            rag[4] = tmpA1.x; rag[5] = tmpA1.y; rag[6] = tmpA1.z; rag[7] = tmpA1.w;\n",
    "\n",
    "            float4 tmpB0 = reinterpret_cast<const float4*>(&Bs[k][tx * TN])[0];\n",
    "            float4 tmpB1 = reinterpret_cast<const float4*>(&Bs[k][tx * TN + 4])[0];\n",
    "            rbg[0] = tmpB0.x; rbg[1] = tmpB0.y; rbg[2] = tmpB0.z; rbg[3] = tmpB0.w;\n",
    "            rbg[4] = tmpB1.x; rbg[5] = tmpB1.y; rbg[6] = tmpB1.z; rbg[7] = tmpB1.w;\n",
    "\n",
    "            #pragma unroll\n",
    "            for (int r = 0; r < TM; ++r) {\n",
    "                #pragma unroll\n",
    "                for (int c = 0; c < TN; ++c) {\n",
    "                    accum[r][c] += rag[r] * rbg[c];\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        __syncthreads();\n",
    "    }\n",
    "\n",
    "    // Write back\n",
    "    int global_row_start = by * BM + ty * TM;\n",
    "    int global_col_start = bx * BN + tx * TN;\n",
    "\n",
    "    #pragma unroll\n",
    "    for (int r = 0; r < TM; ++r) {\n",
    "        int global_r = global_row_start + r;\n",
    "        if (global_r < M) {\n",
    "            int global_c = global_col_start;\n",
    "            if (global_c + 7 < N) {\n",
    "                float4 tmp0, tmp1;\n",
    "                tmp0.x = accum[r][0]; tmp0.y = accum[r][1]; tmp0.z = accum[r][2]; tmp0.w = accum[r][3];\n",
    "                tmp1.x = accum[r][4]; tmp1.y = accum[r][5]; tmp1.z = accum[r][6]; tmp1.w = accum[r][7];\n",
    "                reinterpret_cast<float4*>(&C[global_r * N + global_c])[0] = tmp0;\n",
    "                reinterpret_cast<float4*>(&C[global_r * N + global_c + 4])[0] = tmp1;\n",
    "            } else {\n",
    "                for (int c = 0; c < TN; ++c) {\n",
    "                    if (global_c + c < N) {\n",
    "                        C[global_r * N + global_c + c] = accum[r][c];\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "// C++ 调用接口\n",
    "extern \"C\" void solve(const float* A, const float* B, float* C, int M, int N, int K) {\n",
    "    dim3 block(256);\n",
    "    dim3 grid((N + BN - 1) / BN, (M + BM - 1) / BM);\n",
    "    sgemm_optimized_t4_kernel<<<grid, block>>>(A, B, C, M, N, K);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa7bf83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting matmul_v7.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile matmul_v7.cu\n",
    "// 第7个版本\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "// ==========================================\n",
    "// T4 优化参数 + 双重缓冲策略\n",
    "// ==========================================\n",
    "const int BM = 128;\n",
    "const int BN = 128;\n",
    "const int BK = 8;\n",
    "const int TM = 8;\n",
    "const int TN = 8;\n",
    "\n",
    "__global__ __launch_bounds__(256)\n",
    "void sgemm_double_buffer_t4(\n",
    "    const float* __restrict__ A, \n",
    "    const float* __restrict__ B, \n",
    "    float* __restrict__ C, \n",
    "    int M, int N, int K) \n",
    "{\n",
    "    int tid = threadIdx.x;\n",
    "    int ty = tid / 16;\n",
    "    int tx = tid % 16;\n",
    "    int by = blockIdx.y;\n",
    "    int bx = blockIdx.x;\n",
    "    \n",
    "    // =========================================================\n",
    "    // 关键改变 1: 双重缓冲 Shared Memory\n",
    "    // 使用 [2] 个 buffer，write_stage 用于写，read_stage 用于算\n",
    "    // =========================================================\n",
    "    __shared__ float As[2][BK][BM]; \n",
    "    __shared__ float Bs[2][BK][BN];\n",
    "\n",
    "    float accum[TM][TN] = {0.0f};\n",
    "    \n",
    "    // 寄存器缓存，用于计算\n",
    "    float rag[TM];\n",
    "    float rbg[TN];\n",
    "\n",
    "    // 寄存器缓存，用于 Global Memory 预取 (Prefetch)\n",
    "    // 每个线程搬运 1 个 float4 的 A 和 1 个 float4 的 B\n",
    "    float4 load_a_reg; \n",
    "    float4 load_b_reg;\n",
    "\n",
    "    const float* A_ptr = A + by * BM * K;\n",
    "    const float* B_ptr = B + bx * BN;\n",
    "\n",
    "    // 加载索引计算\n",
    "    int load_a_row = tid / 2; \n",
    "    int load_a_col = (tid % 2) * 4;\n",
    "    int load_b_row = tid / 32;\n",
    "    int load_b_col = (tid % 32) * 4;\n",
    "\n",
    "    // =========================================================\n",
    "    // Prologue (序幕): 加载第一个 Tile 到 Buffer 0\n",
    "    // =========================================================\n",
    "    {\n",
    "        int k_start = 0;\n",
    "        // Load A\n",
    "        if (by * BM + load_a_row < M && k_start + load_a_col < K) {\n",
    "            load_a_reg = reinterpret_cast<const float4*>(&A_ptr[load_a_row * K + k_start + load_a_col])[0];\n",
    "        } else {\n",
    "            load_a_reg = {0.0f, 0.0f, 0.0f, 0.0f};\n",
    "        }\n",
    "        // Load B\n",
    "        if (k_start + load_b_row < K && bx * BN + load_b_col < N) {\n",
    "            load_b_reg = reinterpret_cast<const float4*>(&B_ptr[(k_start + load_b_row) * N + load_b_col])[0];\n",
    "        } else {\n",
    "            load_b_reg = {0.0f, 0.0f, 0.0f, 0.0f};\n",
    "        }\n",
    "\n",
    "        // 写入 SMEM Buffer 0\n",
    "        // A 转置写入\n",
    "        As[0][load_a_col + 0][load_a_row] = load_a_reg.x;\n",
    "        As[0][load_a_col + 1][load_a_row] = load_a_reg.y;\n",
    "        As[0][load_a_col + 2][load_a_row] = load_a_reg.z;\n",
    "        As[0][load_a_col + 3][load_a_row] = load_a_reg.w;\n",
    "        \n",
    "        // B 直接写入\n",
    "        reinterpret_cast<float4*>(&Bs[0][load_b_row][load_b_col])[0] = load_b_reg;\n",
    "    }\n",
    "\n",
    "    __syncthreads();\n",
    "\n",
    "    // =========================================================\n",
    "    // Main Loop\n",
    "    // =========================================================\n",
    "    int write_stage_idx = 1; // 下一轮写入的位置\n",
    "    int read_stage_idx = 0;  // 当前计算读取的位置\n",
    "\n",
    "    // 注意：循环从 k=0 开始算，但在 k 时我们要预加载 k+BK 的数据\n",
    "    for (int k = 0; k < K; k += BK) {\n",
    "        \n",
    "        // -----------------------------------------------------\n",
    "        // 1. Prefetch Next Tile to Registers (Global -> Register)\n",
    "        // 这里的关键是：当我们发起 Global Load 指令后，GPU 不会阻塞，\n",
    "        // 而是会继续向下执行计算指令 (Math)，从而隐藏内存延迟。\n",
    "        // -----------------------------------------------------\n",
    "        int next_k = k + BK;\n",
    "        if (next_k < K) {\n",
    "            // Load A to Reg\n",
    "            if (by * BM + load_a_row < M && next_k + load_a_col < K) {\n",
    "                load_a_reg = reinterpret_cast<const float4*>(&A_ptr[load_a_row * K + next_k + load_a_col])[0];\n",
    "            } else {\n",
    "                load_a_reg = {0.0f, 0.0f, 0.0f, 0.0f};\n",
    "            }\n",
    "            // Load B to Reg\n",
    "            if (next_k + load_b_row < K && bx * BN + load_b_col < N) {\n",
    "                load_b_reg = reinterpret_cast<const float4*>(&B_ptr[(next_k + load_b_row) * N + load_b_col])[0];\n",
    "            } else {\n",
    "                load_b_reg = {0.0f, 0.0f, 0.0f, 0.0f};\n",
    "            }\n",
    "        }\n",
    "\n",
    "        // -----------------------------------------------------\n",
    "        // 2. Compute Current Tile (Register <-> SMEM)\n",
    "        // 使用 read_stage_idx\n",
    "        // -----------------------------------------------------\n",
    "        #pragma unroll\n",
    "        for (int i = 0; i < BK; ++i) {\n",
    "            // Load A from SMEM to Reg\n",
    "            float4 tmpA0 = reinterpret_cast<const float4*>(&As[read_stage_idx][i][ty * TM])[0];\n",
    "            float4 tmpA1 = reinterpret_cast<const float4*>(&As[read_stage_idx][i][ty * TM + 4])[0];\n",
    "            rag[0] = tmpA0.x; rag[1] = tmpA0.y; rag[2] = tmpA0.z; rag[3] = tmpA0.w;\n",
    "            rag[4] = tmpA1.x; rag[5] = tmpA1.y; rag[6] = tmpA1.z; rag[7] = tmpA1.w;\n",
    "\n",
    "            // Load B from SMEM to Reg\n",
    "            float4 tmpB0 = reinterpret_cast<const float4*>(&Bs[read_stage_idx][i][tx * TN])[0];\n",
    "            float4 tmpB1 = reinterpret_cast<const float4*>(&Bs[read_stage_idx][i][tx * TN + 4])[0];\n",
    "            rbg[0] = tmpB0.x; rbg[1] = tmpB0.y; rbg[2] = tmpB0.z; rbg[3] = tmpB0.w;\n",
    "            rbg[4] = tmpB1.x; rbg[5] = tmpB1.y; rbg[6] = tmpB1.z; rbg[7] = tmpB1.w;\n",
    "\n",
    "            // Compute\n",
    "            #pragma unroll\n",
    "            for (int r = 0; r < TM; ++r) {\n",
    "                #pragma unroll\n",
    "                for (int c = 0; c < TN; ++c) {\n",
    "                    accum[r][c] += rag[r] * rbg[c];\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        // -----------------------------------------------------\n",
    "        // 3. Store Prefetched Data to SMEM (Register -> SMEM)\n",
    "        // 此时计算已经完成，我们等待所有线程都算完了当前块\n",
    "        // -----------------------------------------------------\n",
    "        __syncthreads(); // 确保 read_stage 的数据大家都不用了\n",
    "\n",
    "        if (next_k < K) {\n",
    "            // 将寄存器里的下一块数据写入 write_stage 的 SMEM\n",
    "            // A Transposed\n",
    "            As[write_stage_idx][load_a_col + 0][load_a_row] = load_a_reg.x;\n",
    "            As[write_stage_idx][load_a_col + 1][load_a_row] = load_a_reg.y;\n",
    "            As[write_stage_idx][load_a_col + 2][load_a_row] = load_a_reg.z;\n",
    "            As[write_stage_idx][load_a_col + 3][load_a_row] = load_a_reg.w;\n",
    "\n",
    "            // B Direct\n",
    "            reinterpret_cast<float4*>(&Bs[write_stage_idx][load_b_row][load_b_col])[0] = load_b_reg;\n",
    "        }\n",
    "\n",
    "        // 翻转 buffer 索引\n",
    "        // write_stage: 1 -> 0 -> 1\n",
    "        // read_stage:  0 -> 1 -> 0\n",
    "        write_stage_idx ^= 1;\n",
    "        read_stage_idx ^= 1;\n",
    "\n",
    "        __syncthreads(); // 确保 write_stage 的数据已经写好，可以作为下一轮的 read_stage\n",
    "    }\n",
    "\n",
    "    // =========================================================\n",
    "    // Write Back\n",
    "    // =========================================================\n",
    "    int global_row_start = by * BM + ty * TM;\n",
    "    int global_col_start = bx * BN + tx * TN;\n",
    "\n",
    "    #pragma unroll\n",
    "    for (int r = 0; r < TM; ++r) {\n",
    "        int global_r = global_row_start + r;\n",
    "        if (global_r < M) {\n",
    "            int global_c = global_col_start;\n",
    "            if (global_c + 7 < N) {\n",
    "                float4 tmp0, tmp1;\n",
    "                tmp0.x = accum[r][0]; tmp0.y = accum[r][1]; tmp0.z = accum[r][2]; tmp0.w = accum[r][3];\n",
    "                tmp1.x = accum[r][4]; tmp1.y = accum[r][5]; tmp1.z = accum[r][6]; tmp1.w = accum[r][7];\n",
    "                reinterpret_cast<float4*>(&C[global_r * N + global_c])[0] = tmp0;\n",
    "                reinterpret_cast<float4*>(&C[global_r * N + global_c + 4])[0] = tmp1;\n",
    "            } else {\n",
    "                for (int c = 0; c < TN; ++c) {\n",
    "                    if (global_c + c < N) {\n",
    "                        C[global_r * N + global_c + c] = accum[r][c];\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "// 宿主调用\n",
    "extern \"C\" void solve(const float* A, const float* B, float* C, int M, int N, int K) {\n",
    "    dim3 block(256);\n",
    "    dim3 grid((N + BN - 1) / BN, (M + BM - 1) / BM);\n",
    "    sgemm_double_buffer_t4<<<grid, block>>>(A, B, C, M, N, K);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faede5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting matmul_v8.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile matmul_v8.cu\n",
    "#include <cuda_runtime.h>\n",
    "#include <iostream>\n",
    "#include <vector>\n",
    "#include <chrono>\n",
    "#include <random>\n",
    "\n",
    "// ==========================================\n",
    "// 核心超参数配置 (针对 T4 调优)\n",
    "// ==========================================\n",
    "const int BM = 128; // Block size M\n",
    "const int BN = 128; // Block size N\n",
    "const int BK = 8;   // K dimension step (K-blocking)\n",
    "const int TM = 8;   // Thread tile M (每个线程计算的行数)\n",
    "const int TN = 8;   // Thread tile N (每个线程计算的列数)\n",
    "\n",
    "// 检查参数是否合理：Block内的线程数 = (BM * BN) / (TM * TN)\n",
    "// 这里: (128*128) / (8*8) = 16384 / 64 = 256 线程/Block\n",
    "// 这是一个非常经典的配置\n",
    "\n",
    "// ==========================================\n",
    "// 辅助宏与函数\n",
    "// ==========================================\n",
    "#define CHECK_CUDA(func)                                                       \\\n",
    "    {                                                                          \\\n",
    "        cudaError_t status = (func);                                           \\\n",
    "        if (status != cudaSuccess) {                                           \\\n",
    "            std::cerr << \"CUDA Error: \" << cudaGetErrorString(status) << \" at line \" << __LINE__ << std::endl; \\\n",
    "            exit(EXIT_FAILURE);                                                \\\n",
    "        }                                                                      \\\n",
    "    }\n",
    "\n",
    "// ==========================================\n",
    "// 极致优化的 SGEMM Kernel\n",
    "// ==========================================\n",
    "__global__ void sgemm_optimized_v1(int M, int N, int K, float alpha, const float *A, const float *B, float beta, float *C) {\n",
    "    // 线程索引\n",
    "    const int tx = threadIdx.x;\n",
    "    const int ty = threadIdx.y; // 本实现是一维blockDim，ty通常为0，但在逻辑上我们视为一维索引\n",
    "    const int tid = threadIdx.x; // 0-255\n",
    "\n",
    "    // Block 索引\n",
    "    const int bx = blockIdx.x;\n",
    "    const int by = blockIdx.y;\n",
    "\n",
    "    // Shared Memory 分配\n",
    "    // 使用 float4 向量化加载，所以不需要为了 float4 特意填充，但为了避免 bank conflict 可能需要 padding\n",
    "    // A tile: BM x BK. B tile: BK x BN.\n",
    "    __shared__ float s_a[BK][BM]; // 转置存储 A，方便后续读取\n",
    "    __shared__ float s_b[BK][BN]; \n",
    "\n",
    "    // 寄存器文件 (每个线程的累加器)\n",
    "    float r_c[TM][TN] = {0.0f};\n",
    "    \n",
    "    // 寄存器缓存，用于从 smem 读取操作数，减少 smem 压力\n",
    "    float r_a[TM];\n",
    "    float r_b[TN];\n",
    "\n",
    "    // 计算当前 Block 需要处理的全局内存 A 和 B 的起始位置\n",
    "    // A: 行由 by 决定 -> by * 128\n",
    "    // B: 列由 bx 决定 -> bx * 128\n",
    "    const float* A_ptr = A + by * BM * K;\n",
    "    const float* B_ptr = B + bx * BN;\n",
    "    float* C_ptr = C + (by * BM * N + bx * BN);\n",
    "\n",
    "    // -------------------------------------------------------\n",
    "    // 向量化加载计算逻辑\n",
    "    // -------------------------------------------------------\n",
    "    // 每个 Block 有 256 个线程。\n",
    "    // 加载 A (BM x BK = 128 x 8 = 1024 floats). 256 线程，每个线程加载 4 个 float.\n",
    "    // 正好一个 float4。\n",
    "    \n",
    "    // A 的加载索引计算:\n",
    "    // 我们想把 A[Row][Col] 加载到 s_a[Col][Row] (转置) 或者 s_a[Row][Col]\n",
    "    // 为了外积计算方便，我们通常希望 A 是列优先或转置的，B 是行优先的\n",
    "    \n",
    "    // 线程负责加载 A 的位置:\n",
    "    // tid 0-255. \n",
    "    // load_a_row = tid / (BK/4)  (但BK=8, BK/4=2) -> tid / 2\n",
    "    // load_a_col = (tid % 2) * 4\n",
    "    // 这种映射比较复杂，我们需要更直观的映射。\n",
    "    \n",
    "    // 简化：我们将 A 视为 [BM, BK] 的矩阵。\n",
    "    // 线程 k 加载 float4 from A memory.\n",
    "    // 每一个线程加载一个 float4。\n",
    "    // A_tile 共有 128行 * 8列。\n",
    "    // 如果按行加载：每行需要 2个 float4 (8 floats)。\n",
    "    // 128行 * 2 = 256 float4。正好对应 256 个线程。\n",
    "    \n",
    "    int load_a_row = tid / 2; \n",
    "    int load_a_col = (tid % 2) * 4;\n",
    "\n",
    "    // B 的加载索引计算:\n",
    "    // B_tile 共有 8行 * 128列。\n",
    "    // 每行 128 floats = 32 个 float4。\n",
    "    // 8行 * 32 = 256 float4。正好对应 256 个线程。\n",
    "    int load_b_row = tid / 32;\n",
    "    int load_b_col = (tid % 32) * 4;\n",
    "\n",
    "    // -------------------------------------------------------\n",
    "    // 主循环：在 K 维度上步进\n",
    "    // -------------------------------------------------------\n",
    "    for (int k = 0; k < K; k += BK) {\n",
    "        // 1. 从 Global Memory 加载数据到 Shared Memory (向量化加载)\n",
    "        \n",
    "        // 加载 A\n",
    "        float4 tmp_a = reinterpret_cast<const float4*>(&A_ptr[load_a_row * K + load_a_col + k])[0];\n",
    "        // 存入 Shared Memory (注意转置，把 A 转置存，有利于计算时避免 Bank Conflict)\n",
    "        // s_a[col][row]\n",
    "        s_a[load_a_col + 0][load_a_row] = tmp_a.x;\n",
    "        s_a[load_a_col + 1][load_a_row] = tmp_a.y;\n",
    "        s_a[load_a_col + 2][load_a_row] = tmp_a.z;\n",
    "        s_a[load_a_col + 3][load_a_row] = tmp_a.w;\n",
    "\n",
    "        // 加载 B\n",
    "        float4 tmp_b = reinterpret_cast<const float4*>(&B_ptr[load_b_row * N + load_b_col])[0];\n",
    "        // 存入 Shared Memory\n",
    "        // B 不需要转置，直接存 [row][col]\n",
    "        reinterpret_cast<float4*>(&s_b[load_b_row][load_b_col])[0] = tmp_b;\n",
    "\n",
    "        // 等待所有线程加载完毕\n",
    "        __syncthreads();\n",
    "\n",
    "        // 2. 计算 (核心 Math Loop)\n",
    "        // BK = 8，循环 8 次\n",
    "        #pragma unroll\n",
    "        for (int i = 0; i < BK; ++i) {\n",
    "            // 将 SMEM 数据读入寄存器\n",
    "            \n",
    "            // 线程计算位置确定:\n",
    "            // 256个线程视作 16x16 的网格。\n",
    "            // Block总大小 128x128。\n",
    "            // 每个线程负责 8x8 的区域 (TMxTN)。\n",
    "            // thread_row = (tid / 16) * 8\n",
    "            // thread_col = (tid % 16) * 8\n",
    "            \n",
    "            int thread_row_start = (tid / 16) * TM;\n",
    "            int thread_col_start = (tid % 16) * TN;\n",
    "\n",
    "            // 加载 A 的一列 (长度 TM=8) 到寄存器\n",
    "            // 由于 s_a 是转置存储的 [BK][BM]，所以这里取 s_a[i][row] 实际上是内存连续的\n",
    "            // 这就是为什么要转置 A 的原因 -> 广播机制\n",
    "            #pragma unroll\n",
    "            for (int r = 0; r < TM; ++r) {\n",
    "                r_a[r] = s_a[i][thread_row_start + r];\n",
    "            }\n",
    "\n",
    "            // 加载 B 的一行 (长度 TN=8) 到寄存器\n",
    "            #pragma unroll\n",
    "            for (int c = 0; c < TN; ++c) {\n",
    "                r_b[c] = s_b[i][thread_col_start + c];\n",
    "            }\n",
    "\n",
    "            // 外积计算 (Outer Product)\n",
    "            #pragma unroll\n",
    "            for (int r = 0; r < TM; ++r) {\n",
    "                #pragma unroll\n",
    "                for (int c = 0; c < TN; ++c) {\n",
    "                    r_c[r][c] += r_a[r] * r_b[c]; // FMA 指令\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        // 同步，准备加载下一块\n",
    "        __syncthreads();\n",
    "        \n",
    "        // 移动 B 的指针 (A 的指针在索引里加 k 处理了，这里不需要动，或者需要根据逻辑调整)\n",
    "        // 其实这里 A_ptr 应该是不动的，因为我们在索引里用了 +k\n",
    "        // 但是 B_ptr 需要移动到下一块 K\n",
    "        B_ptr += BK * N; \n",
    "        // A_ptr 其实不需要动，因为上面是 load_a_row * K + k。如果这里不动，上面就要 k += BK\n",
    "    }\n",
    "\n",
    "    // 3. 将结果写回 Global Memory\n",
    "    // 同样需要向量化写回 (如果可能)\n",
    "    int thread_row_start = (tid / 16) * TM;\n",
    "    int thread_col_start = (tid % 16) * TN;\n",
    "\n",
    "    #pragma unroll\n",
    "    for (int r = 0; r < TM; ++r) {\n",
    "        #pragma unroll\n",
    "        for (int c = 0; c < TN; c += 4) {\n",
    "            // 计算全局坐标\n",
    "            int global_row = by * BM + thread_row_start + r;\n",
    "            int global_col = bx * BN + thread_col_start + c;\n",
    "            \n",
    "            // 这里的写入由于 C 的布局是行优先，\n",
    "            // 线程 (r, c) 处理的连续 4 个 float 正好是内存连续的，可以用 float4 写入\n",
    "            \n",
    "            float4 val;\n",
    "            val.x = alpha * r_c[r][c + 0] + beta * C_ptr[(thread_row_start + r) * N + thread_col_start + c + 0];\n",
    "            val.y = alpha * r_c[r][c + 1] + beta * C_ptr[(thread_row_start + r) * N + thread_col_start + c + 1];\n",
    "            val.z = alpha * r_c[r][c + 2] + beta * C_ptr[(thread_row_start + r) * N + thread_col_start + c + 2];\n",
    "            val.w = alpha * r_c[r][c + 3] + beta * C_ptr[(thread_row_start + r) * N + thread_col_start + c + 3];\n",
    "\n",
    "            // 写入\n",
    "            reinterpret_cast<float4*>(&C[global_row * N + global_col])[0] = val;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "// 宿主调用\n",
    "extern \"C\" void solve(const float* A, const float* B, float* C, int M, int N, int K) {\n",
    "    dim3 block(256);\n",
    "    dim3 grid(N / BN, M / BM);\n",
    "\n",
    "    // Warmup\n",
    "    sgemm_optimized_v1<<<grid, block>>>(M, N, K, 1.0f, A, B, 0.0f, C);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a2f9c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "matmul_v8.cu(37): warning #177-D: variable \"tx\" was declared but never referenced\n",
      "      const int tx = threadIdx.x;\n",
      "                ^\n",
      "\n",
      "Remark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
      "\n",
      "matmul_v8.cu(38): warning #177-D: variable \"ty\" was declared but never referenced\n",
      "      const int ty = threadIdx.y;\n",
      "                ^\n",
      "\n",
      "matmul_v5.cu(55): warning #177-D: variable \"C_ptr\" was declared but never referenced\n",
      "      float* C_ptr = C + by * BM * N + bx * BN;\n",
      "             ^\n",
      "\n",
      "Remark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
      "\n",
      "matmul_v5.cu(206): warning #177-D: variable \"N\" was declared but never referenced\n",
      "      int N = 4096;\n",
      "          ^\n",
      "\n",
      "matmul_v5.cu(208): warning #177-D: variable \"size\" was declared but never referenced\n",
      "      size_t size = M * K * sizeof(float);\n",
      "             ^\n",
      "\n",
      "matmul_v5.cu(210): warning #177-D: variable \"h_A\" was declared but never referenced\n",
      "      float *h_A, *h_B, *h_C, *d_A, *d_B, *d_C;\n",
      "             ^\n",
      "\n",
      "matmul_v5.cu(210): warning #177-D: variable \"h_B\" was declared but never referenced\n",
      "      float *h_A, *h_B, *h_C, *d_A, *d_B, *d_C;\n",
      "                   ^\n",
      "\n",
      "matmul_v5.cu(210): warning #177-D: variable \"h_C\" was declared but never referenced\n",
      "      float *h_A, *h_B, *h_C, *d_A, *d_B, *d_C;\n",
      "                         ^\n",
      "\n",
      "matmul_v5.cu(210): warning #177-D: variable \"d_A\" was declared but never referenced\n",
      "      float *h_A, *h_B, *h_C, *d_A, *d_B, *d_C;\n",
      "                               ^\n",
      "\n",
      "matmul_v5.cu(210): warning #177-D: variable \"d_B\" was declared but never referenced\n",
      "      float *h_A, *h_B, *h_C, *d_A, *d_B, *d_C;\n",
      "                                     ^\n",
      "\n",
      "matmul_v5.cu(210): warning #177-D: variable \"d_C\" was declared but never referenced\n",
      "      float *h_A, *h_B, *h_C, *d_A, *d_B, *d_C;\n",
      "                                           ^\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['matmul_v3.so', 'matmul_v6.cu', 'matmul_v5.so', 'matmul_v8.cu', 'matmul_v5.cu', 'matmul_v2.cu', 'matmul_v7.cu', 'matmul_v3.cu', 'matmul_v6.so', 'matmul_v4_1.cu', 'matmul_v7.so', 'matmul.cu', 'build', '.virtual_documents', 'setup.py', 'matmul_v8.so', 'matmul_v4.so', 'matmul.so', 'matmul_v4_1.so', 'matmul_v2.so', 'matmul_v4.cu']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "for v in os.listdir(os.path.abspath('.')):\n",
    "    prefix, end = os.path.splitext(v)\n",
    "    if end == '.cu':\n",
    "        subprocess.run(f\"nvcc -shared -o {prefix}.so {prefix}.cu -Xcompiler -fPIC\", shell=True)\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43a97eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在进行矩阵乘法测试: [1024x1024] * [1024x1024]\n",
      "Pytorch: \n",
      "591 µs ± 2.67 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "\n",
      "matmul.so\n",
      "✅ 测试通过！结果正确。\n",
      "4.38 ms ± 14.5 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "matmul_v2.so\n",
      "✅ 测试通过！结果正确。\n",
      "2.42 ms ± 4.57 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "matmul_v3.so\n",
      "✅ 测试通过！结果正确。\n",
      "2.4 ms ± 9.49 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "matmul_v4.so\n",
      "✅ 测试通过！结果正确。\n",
      "783 µs ± 4.83 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "\n",
      "matmul_v4_1.so\n",
      "✅ 测试通过！结果正确。\n",
      "790 µs ± 5.09 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "\n",
      "matmul_v5.so\n",
      "✅ 测试通过！结果正确。\n",
      "694 µs ± 849 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "\n",
      "matmul_v6.so\n",
      "✅ 测试通过！结果正确。\n",
      "696 µs ± 1.03 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "\n",
      "matmul_v7.so\n",
      "✅ 测试通过！结果正确。\n",
      "711 µs ± 1.21 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "\n",
      "matmul_v8.so\n",
      "✅ 测试通过！结果正确。\n",
      "722 µs ± 1.61 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import ctypes\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# --- 测试部分 ---\n",
    "\n",
    "M, N, K = 1024, 1024, 1024\n",
    "\n",
    "print(f\"正在进行矩阵乘法测试: [{M}x{N}] * [{N}x{K}]\")\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "A = torch.randn(M, N, device=device, dtype=torch.float32)\n",
    "B = torch.randn(N, K, device=device, dtype=torch.float32)\n",
    "\n",
    "C_torch = torch.matmul(A, B)\n",
    "\n",
    "print(\"Pytorch: \")\n",
    "%timeit torch.matmul(A, B); torch.cuda.synchronize()\n",
    "print()\n",
    "\n",
    "for v in sorted(os.listdir('.')):\n",
    "    prefix, end = os.path.splitext(v)\n",
    "    \n",
    "    if end != '.so':\n",
    "        continue\n",
    "\n",
    "    lib = ctypes.CDLL(f'./{v}')\n",
    "\n",
    "    # void solve(const float* A, const float* B, float* C, int M, int N, int K)\n",
    "    # 指针对应 c_void_p, int 对应 c_int\n",
    "    lib.solve.argtypes = [\n",
    "        ctypes.c_void_p, \n",
    "        ctypes.c_void_p, \n",
    "        ctypes.c_void_p, \n",
    "        ctypes.c_int, \n",
    "        ctypes.c_int, \n",
    "        ctypes.c_int\n",
    "    ]\n",
    "\n",
    "    def cuda_matmul(a_tensor, b_tensor):\n",
    "        M, N = a_tensor.shape\n",
    "        N_b, K = b_tensor.shape\n",
    "        \n",
    "        assert N == N_b, f\"矩阵维度不匹配: {N} != {N_b}\"\n",
    "        \n",
    "        c_tensor = torch.zeros((M, K), device='cuda', dtype=torch.float32)\n",
    "        \n",
    "        if not a_tensor.is_contiguous(): a_tensor = a_tensor.contiguous()\n",
    "        if not b_tensor.is_contiguous(): b_tensor = b_tensor.contiguous()\n",
    "        \n",
    "        lib.solve(\n",
    "            ctypes.c_void_p(a_tensor.data_ptr()),\n",
    "            ctypes.c_void_p(b_tensor.data_ptr()),\n",
    "            ctypes.c_void_p(c_tensor.data_ptr()),\n",
    "            ctypes.c_int(M),\n",
    "            ctypes.c_int(N),\n",
    "            ctypes.c_int(K)\n",
    "        )\n",
    "        \n",
    "        return c_tensor\n",
    "\n",
    "    C_custom = cuda_matmul(A, B)\n",
    "    print(f\"{v}\")\n",
    "    if torch.allclose(C_custom, C_torch, atol=1e-3):\n",
    "        print(f\"✅ 测试通过！结果正确。\")\n",
    "        %timeit cuda_matmul(A, B); torch.cuda.synchronize()\n",
    "    else:\n",
    "        print(\"❌ 测试失败。结果不一致。\")\n",
    "        print(\"最大误差:\", (C_custom - C_torch).abs().max().item())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dd1ad77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在进行矩阵乘法测试: [2048x2048] * [2048x2048]\n",
      "Pytorch: \n",
      "4.8 ms ± 30.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "matmul.so\n",
      "✅ 测试通过！结果正确。\n",
      "44 ms ± 405 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "\n",
      "matmul_v2.so\n",
      "✅ 测试通过！结果正确。\n",
      "21.1 ms ± 265 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "\n",
      "matmul_v3.so\n",
      "✅ 测试通过！结果正确。\n",
      "21.5 ms ± 80.6 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "\n",
      "matmul_v4.so\n",
      "✅ 测试通过！结果正确。\n",
      "6.81 ms ± 14.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "matmul_v4_1.so\n",
      "✅ 测试通过！结果正确。\n",
      "6.91 ms ± 19.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "matmul_v5.so\n",
      "✅ 测试通过！结果正确。\n",
      "5.31 ms ± 7.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "matmul_v6.so\n",
      "✅ 测试通过！结果正确。\n",
      "5.37 ms ± 21.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "matmul_v7.so\n",
      "✅ 测试通过！结果正确。\n",
      "5.49 ms ± 8.09 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "matmul_v8.so\n",
      "✅ 测试通过！结果正确。\n",
      "5.52 ms ± 15.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import ctypes\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# --- 测试部分 ---\n",
    "\n",
    "M, N, K = 2048, 2048, 2048\n",
    "\n",
    "print(f\"正在进行矩阵乘法测试: [{M}x{N}] * [{N}x{K}]\")\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "A = torch.randn(M, N, device=device, dtype=torch.float32)\n",
    "B = torch.randn(N, K, device=device, dtype=torch.float32)\n",
    "\n",
    "C_torch = torch.matmul(A, B)\n",
    "\n",
    "print(\"Pytorch: \")\n",
    "%timeit torch.matmul(A, B); torch.cuda.synchronize()\n",
    "print()\n",
    "\n",
    "for v in sorted(os.listdir('.')):\n",
    "    prefix, end = os.path.splitext(v)\n",
    "    \n",
    "    if end != '.so':\n",
    "        continue\n",
    "\n",
    "    lib = ctypes.CDLL(f'./{v}')\n",
    "\n",
    "    # void solve(const float* A, const float* B, float* C, int M, int N, int K)\n",
    "    # 指针对应 c_void_p, int 对应 c_int\n",
    "    lib.solve.argtypes = [\n",
    "        ctypes.c_void_p, \n",
    "        ctypes.c_void_p, \n",
    "        ctypes.c_void_p, \n",
    "        ctypes.c_int, \n",
    "        ctypes.c_int, \n",
    "        ctypes.c_int\n",
    "    ]\n",
    "\n",
    "    def cuda_matmul(a_tensor, b_tensor):\n",
    "        M, N = a_tensor.shape\n",
    "        N_b, K = b_tensor.shape\n",
    "        \n",
    "        assert N == N_b, f\"矩阵维度不匹配: {N} != {N_b}\"\n",
    "        \n",
    "        c_tensor = torch.zeros((M, K), device='cuda', dtype=torch.float32)\n",
    "        \n",
    "        if not a_tensor.is_contiguous(): a_tensor = a_tensor.contiguous()\n",
    "        if not b_tensor.is_contiguous(): b_tensor = b_tensor.contiguous()\n",
    "        \n",
    "        lib.solve(\n",
    "            ctypes.c_void_p(a_tensor.data_ptr()),\n",
    "            ctypes.c_void_p(b_tensor.data_ptr()),\n",
    "            ctypes.c_void_p(c_tensor.data_ptr()),\n",
    "            ctypes.c_int(M),\n",
    "            ctypes.c_int(N),\n",
    "            ctypes.c_int(K)\n",
    "        )\n",
    "        \n",
    "        return c_tensor\n",
    "\n",
    "    C_custom = cuda_matmul(A, B)\n",
    "    print(f\"{v}\")\n",
    "    if torch.allclose(C_custom, C_torch, atol=1e-3):\n",
    "        print(f\"✅ 测试通过！结果正确。\")\n",
    "        %timeit cuda_matmul(A, B); torch.cuda.synchronize()\n",
    "    else:\n",
    "        print(\"❌ 测试失败。结果不一致。\")\n",
    "        print(\"最大误差:\", (C_custom - C_torch).abs().max().item())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a252a9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在进行矩阵乘法测试: [4096x4096] * [4096x4096]\n",
      "Pytorch: \n",
      "42.1 ms ± 132 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "\n",
      "matmul.so\n",
      "✅ 测试通过！结果正确。\n",
      "390 ms ± 1.26 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "\n",
      "matmul_v2.so\n",
      "✅ 测试通过！结果正确。\n",
      "184 ms ± 1.42 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "\n",
      "matmul_v3.so\n",
      "✅ 测试通过！结果正确。\n",
      "181 ms ± 1.44 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "\n",
      "matmul_v4.so\n",
      "✅ 测试通过！结果正确。\n",
      "59.9 ms ± 374 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "\n",
      "matmul_v4_1.so\n",
      "✅ 测试通过！结果正确。\n",
      "59.1 ms ± 99 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "\n",
      "matmul_v5.so\n",
      "✅ 测试通过！结果正确。\n",
      "43.6 ms ± 194 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "\n",
      "matmul_v6.so\n",
      "✅ 测试通过！结果正确。\n",
      "43.4 ms ± 117 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "\n",
      "matmul_v7.so\n",
      "✅ 测试通过！结果正确。\n",
      "43.7 ms ± 76.6 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "\n",
      "matmul_v8.so\n",
      "✅ 测试通过！结果正确。\n",
      "43.1 ms ± 73.2 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import ctypes\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# --- 测试部分 ---\n",
    "\n",
    "M, N, K = 4096, 4096, 4096\n",
    "\n",
    "print(f\"正在进行矩阵乘法测试: [{M}x{N}] * [{N}x{K}]\")\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "A = torch.randn(M, N, device=device, dtype=torch.float32)\n",
    "B = torch.randn(N, K, device=device, dtype=torch.float32)\n",
    "\n",
    "C_torch = torch.matmul(A, B)\n",
    "\n",
    "print(\"Pytorch: \")\n",
    "%timeit torch.matmul(A, B); torch.cuda.synchronize()\n",
    "print()\n",
    "\n",
    "for v in sorted(os.listdir('.')):\n",
    "    prefix, end = os.path.splitext(v)\n",
    "    \n",
    "    if end != '.so':\n",
    "        continue\n",
    "\n",
    "    lib = ctypes.CDLL(f'./{v}')\n",
    "\n",
    "    # void solve(const float* A, const float* B, float* C, int M, int N, int K)\n",
    "    # 指针对应 c_void_p, int 对应 c_int\n",
    "    lib.solve.argtypes = [\n",
    "        ctypes.c_void_p, \n",
    "        ctypes.c_void_p, \n",
    "        ctypes.c_void_p, \n",
    "        ctypes.c_int, \n",
    "        ctypes.c_int, \n",
    "        ctypes.c_int\n",
    "    ]\n",
    "\n",
    "    def cuda_matmul(a_tensor, b_tensor):\n",
    "        M, N = a_tensor.shape\n",
    "        N_b, K = b_tensor.shape\n",
    "        \n",
    "        assert N == N_b, f\"矩阵维度不匹配: {N} != {N_b}\"\n",
    "        \n",
    "        c_tensor = torch.zeros((M, K), device='cuda', dtype=torch.float32)\n",
    "        \n",
    "        if not a_tensor.is_contiguous(): a_tensor = a_tensor.contiguous()\n",
    "        if not b_tensor.is_contiguous(): b_tensor = b_tensor.contiguous()\n",
    "        \n",
    "        lib.solve(\n",
    "            ctypes.c_void_p(a_tensor.data_ptr()),\n",
    "            ctypes.c_void_p(b_tensor.data_ptr()),\n",
    "            ctypes.c_void_p(c_tensor.data_ptr()),\n",
    "            ctypes.c_int(M),\n",
    "            ctypes.c_int(N),\n",
    "            ctypes.c_int(K)\n",
    "        )\n",
    "        \n",
    "        return c_tensor\n",
    "\n",
    "    C_custom = cuda_matmul(A, B)\n",
    "    print(f\"{v}\")\n",
    "    if torch.allclose(C_custom, C_torch, atol=1e-3):\n",
    "        print(f\"✅ 测试通过！结果正确。\")\n",
    "        %timeit cuda_matmul(A, B); torch.cuda.synchronize()\n",
    "    else:\n",
    "        print(\"❌ 测试失败。结果不一致。\")\n",
    "        print(\"最大误差:\", (C_custom - C_torch).abs().max().item())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b3e612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在进行矩阵乘法测试: [1024x1024] * [1024x1024]\n",
      "Pytorch: \n",
      "569 µs ± 3.44 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "\n",
      "matmul_v8.so\n",
      "✅ 测试通过！结果正确。\n",
      "669 µs ± 1.13 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import ctypes\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# --- 测试部分 ---\n",
    "\n",
    "M, N, K = 1024, 1024, 1024\n",
    "\n",
    "print(f\"正在进行矩阵乘法测试: [{M}x{N}] * [{N}x{K}]\")\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "A = torch.randn(M, N, device=device, dtype=torch.float32)\n",
    "B = torch.randn(N, K, device=device, dtype=torch.float32)\n",
    "\n",
    "C_torch = torch.matmul(A, B)\n",
    "\n",
    "print(\"Pytorch: \")\n",
    "%timeit torch.matmul(A, B); torch.cuda.synchronize()\n",
    "print()\n",
    "\n",
    "v = 'matmul_v8.so'\n",
    "prefix, end = os.path.splitext(v)\n",
    "\n",
    "lib = ctypes.CDLL(f'./{v}')\n",
    "\n",
    "# void solve(const float* A, const float* B, float* C, int M, int N, int K)\n",
    "# 指针对应 c_void_p, int 对应 c_int\n",
    "lib.solve.argtypes = [\n",
    "    ctypes.c_void_p, \n",
    "    ctypes.c_void_p, \n",
    "    ctypes.c_void_p, \n",
    "    ctypes.c_int, \n",
    "    ctypes.c_int, \n",
    "    ctypes.c_int\n",
    "]\n",
    "\n",
    "def cuda_matmul(a_tensor, b_tensor):\n",
    "    M, N = a_tensor.shape\n",
    "    N_b, K = b_tensor.shape\n",
    "    \n",
    "    assert N == N_b, f\"矩阵维度不匹配: {N} != {N_b}\"\n",
    "    \n",
    "    c_tensor = torch.zeros((M, K), device='cuda', dtype=torch.float32)\n",
    "    \n",
    "    if not a_tensor.is_contiguous(): a_tensor = a_tensor.contiguous()\n",
    "    if not b_tensor.is_contiguous(): b_tensor = b_tensor.contiguous()\n",
    "    \n",
    "    lib.solve(\n",
    "        ctypes.c_void_p(a_tensor.data_ptr()),\n",
    "        ctypes.c_void_p(b_tensor.data_ptr()),\n",
    "        ctypes.c_void_p(c_tensor.data_ptr()),\n",
    "        ctypes.c_int(M),\n",
    "        ctypes.c_int(N),\n",
    "        ctypes.c_int(K)\n",
    "    )\n",
    "    \n",
    "    return c_tensor\n",
    "\n",
    "C_custom = cuda_matmul(A, B)\n",
    "print(f\"{v}\")\n",
    "if torch.allclose(C_custom, C_torch, atol=1e-3):\n",
    "    print(f\"✅ 测试通过！结果正确。\")\n",
    "    %timeit cuda_matmul(A, B); torch.cuda.synchronize()\n",
    "else:\n",
    "    print(\"❌ 测试失败。结果不一致。\")\n",
    "    print(\"最大误差:\", (C_custom - C_torch).abs().max().item())\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
