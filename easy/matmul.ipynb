{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "922b3758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting matmul.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile matmul.cu\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "// Kernel 定义\n",
    "__global__ void matrix_multiplication_kernel(const float* A, const float* B, float* C, int M, int N, int K) {\n",
    "    int row =  blockDim.y * blockIdx.y + threadIdx.y;\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    if(col >= K || row >= M){\n",
    "        return;\n",
    "    }\n",
    "    \n",
    "    float acc = 0.0f;\n",
    "    for(int i = 0; i < N; i++){\n",
    "        acc += A[row * N + i] * B[i * K + col];\n",
    "    }\n",
    "    C[row * K + col] = acc; \n",
    "}\n",
    "\n",
    "// 宿主端 wrapper 函数\n",
    "extern \"C\" void solve(const float* A, const float* B, float* C, int M, int N, int K) {\n",
    "    dim3 threadsPerBlock(16, 16);\n",
    "    // 注意：grid 的计算需要向上取整，你的代码已经包含了这个逻辑，但建议加上括号保证运算顺序\n",
    "    dim3 blocksPerGrid((K + threadsPerBlock.x - 1) / threadsPerBlock.x,\n",
    "                       (M + threadsPerBlock.y - 1) / threadsPerBlock.y);\n",
    "    \n",
    "    matrix_multiplication_kernel<<<blocksPerGrid, threadsPerBlock>>>(A, B, C, M, N, K);\n",
    "    \n",
    "    // 检查是否有错误发生（调试用）\n",
    "    cudaError_t err = cudaGetLastError();\n",
    "    if (err != cudaSuccess) {\n",
    "        printf(\"CUDA Error: %s\\n\", cudaGetErrorString(err));\n",
    "    }\n",
    "    \n",
    "    // 等待 GPU 完成\n",
    "    cudaDeviceSynchronize();\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99606131",
   "metadata": {},
   "source": [
    "上面的实现有一个很大的问题就是，在每一个线程里面，为了计算C里面的一个位置上面的值，需要去全局内存上访问col次数据A，也需要访问col次数据B，导致数据等待时间大大增加。解决方法在于，使用线程块里面的共享内存，在一个线程块里面，先读取A和B的一部分数据到线程块里面的共享内存里面，然后进行计算，接着读取下一部分。通过分块避免所有线程去读取全局内存里面的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b98cae66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting matmul_v2.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile matmul_v2.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#define TILE_WIDTH 32 // 假设 BlockDim 为 32x32\n",
    "\n",
    "__global__ void matrix_multiplication_shared_mem(const float* __restrict__ A, const float* __restrict__ B, float* C, int M, int N, int K) {\n",
    "    // 申请共享内存\n",
    "    __shared__ float As[TILE_WIDTH][TILE_WIDTH];\n",
    "    __shared__ float Bs[TILE_WIDTH][TILE_WIDTH];\n",
    "\n",
    "    int bx = blockIdx.x;\n",
    "    int by = blockIdx.y;\n",
    "    int tx = threadIdx.x;\n",
    "    int ty = threadIdx.y;\n",
    "\n",
    "    // 计算当前线程在全局矩阵 C 中负责的坐标\n",
    "    int row = by * TILE_WIDTH + ty;\n",
    "    int col = bx * TILE_WIDTH + tx;\n",
    "\n",
    "    float acc = 0.0f;\n",
    "\n",
    "    // 循环遍历所有的 Tile (以 TILE_WIDTH 为步长遍历 N 维度)\n",
    "    for (int t = 0; t < (N + TILE_WIDTH - 1) / TILE_WIDTH; ++t) {\n",
    "\n",
    "        // 1. 协作加载 A 的 Tile 到共享内存\n",
    "        // 边界检查：防止索引越界 (Padding 0)\n",
    "        if (row < M && t * TILE_WIDTH + tx < N) {\n",
    "            As[ty][tx] = A[row * N + t * TILE_WIDTH + tx];\n",
    "        } else {\n",
    "            As[ty][tx] = 0.0f;\n",
    "        }\n",
    "\n",
    "        // 2. 协作加载 B 的 Tile 到共享内存\n",
    "        if (col < K && t * TILE_WIDTH + ty < N) {\n",
    "            // 注意这里 B 的索引：行是 t*TILE_WIDTH + ty, 列是 col\n",
    "            Bs[ty][tx] = B[(t * TILE_WIDTH + ty) * K + col];\n",
    "        } else {\n",
    "            Bs[ty][tx] = 0.0f;\n",
    "        }\n",
    "\n",
    "        // 3. 必须同步！确保所有线程都加载完了数据\n",
    "        __syncthreads();\n",
    "\n",
    "        // 4. 在共享内存上进行计算\n",
    "        for (int i = 0; i < TILE_WIDTH; ++i) {\n",
    "            acc += As[ty][i] * Bs[i][tx];\n",
    "        }\n",
    "\n",
    "        // 5. 必须同步！确保在加载下一个 Tile 之前，当前 Tile 的数据已经用完\n",
    "        __syncthreads();\n",
    "    }\n",
    "\n",
    "    // 写回结果\n",
    "    if (row < M && col < K) {\n",
    "        C[row * K + col] = acc;\n",
    "    }\n",
    "}\n",
    "\n",
    "// 宿主端 wrapper 函数\n",
    "extern \"C\" void solve(const float* A, const float* B, float* C, int M, int N, int K) {\n",
    "    dim3 threadsPerBlock(TILE_WIDTH, TILE_WIDTH);\n",
    "    // 注意：grid 的计算需要向上取整，你的代码已经包含了这个逻辑，但建议加上括号保证运算顺序\n",
    "    dim3 blocksPerGrid((K + threadsPerBlock.x - 1) / threadsPerBlock.x,\n",
    "                       (M + threadsPerBlock.y - 1) / threadsPerBlock.y);\n",
    "    \n",
    "    matrix_multiplication_shared_mem<<<blocksPerGrid, threadsPerBlock>>>(A, B, C, M, N, K);\n",
    "    \n",
    "    // 检查是否有错误发生（调试用）\n",
    "    cudaError_t err = cudaGetLastError();\n",
    "    if (err != cudaSuccess) {\n",
    "        printf(\"CUDA Error: %s\\n\", cudaGetErrorString(err));\n",
    "    }\n",
    "    \n",
    "    // 等待 GPU 完成\n",
    "    cudaDeviceSynchronize();\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4ee7d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting matmul_v3.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile matmul_v3.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#define TILE_WIDTH 32 // 假设 BlockDim 为 32x32\n",
    "\n",
    "__global__ void matrix_multiplication_shared_mem(const float *__restrict__ A, const float *__restrict__ B, float *C, int M, int N, int K)\n",
    "{\n",
    "    // 申请共享内存\n",
    "    __shared__ float As[TILE_WIDTH][TILE_WIDTH];\n",
    "    __shared__ float Bs[TILE_WIDTH][TILE_WIDTH];\n",
    "\n",
    "    int ty = threadIdx.y;\n",
    "    int tx = threadIdx.x;\n",
    "\n",
    "    int row = blockIdx.y * TILE_WIDTH + ty;\n",
    "    int col = blockIdx.x * TILE_WIDTH + tx;\n",
    "\n",
    "    float acc = 0.0f;\n",
    "\n",
    "    for (int i = 0; i < (N + TILE_WIDTH - 1) / TILE_WIDTH; i++)\n",
    "    {\n",
    "        // 读取数据\n",
    "        if (row < M && TILE_WIDTH * i + tx < N)\n",
    "        {\n",
    "            As[ty][tx] = A[row * N + TILE_WIDTH * i + tx];\n",
    "        }\n",
    "        else\n",
    "        {\n",
    "            As[ty][tx] = 0.0f;\n",
    "        }\n",
    "        if (col < K && TILE_WIDTH * i + ty < N)\n",
    "        {\n",
    "            Bs[tx][ty] = B[(TILE_WIDTH * i + ty) * K + col];\n",
    "        }\n",
    "        else\n",
    "        {\n",
    "            Bs[tx][ty] = 0.0f;\n",
    "        }\n",
    "        __syncthreads(); // 等待线程块里面的线程都搬运完成\n",
    "\n",
    "        for (int j = 0; j < TILE_WIDTH; j++)\n",
    "        {\n",
    "            acc += As[ty][j] * Bs[tx][j]; //访问线程块里面的共享内存时，没有合并访问，只在乎bank config\n",
    "        }\n",
    "\n",
    "        __syncthreads();\n",
    "    }\n",
    "    if (row<M && col<K)\n",
    "    {\n",
    "        C[row*K+col] = acc;\n",
    "    }\n",
    "}\n",
    "\n",
    "// 宿主端 wrapper 函数\n",
    "extern \"C\" void solve(const float *A, const float *B, float *C, int M, int N, int K)\n",
    "{\n",
    "    dim3 threadsPerBlock(TILE_WIDTH, TILE_WIDTH);\n",
    "    // 注意：grid 的计算需要向上取整，你的代码已经包含了这个逻辑，但建议加上括号保证运算顺序\n",
    "    dim3 blocksPerGrid((K + threadsPerBlock.x - 1) / threadsPerBlock.x,\n",
    "                       (M + threadsPerBlock.y - 1) / threadsPerBlock.y);\n",
    "\n",
    "    matrix_multiplication_shared_mem<<<blocksPerGrid, threadsPerBlock>>>(A, B, C, M, N, K);\n",
    "\n",
    "    // 检查是否有错误发生（调试用）\n",
    "    cudaError_t err = cudaGetLastError();\n",
    "    if (err != cudaSuccess)\n",
    "    {\n",
    "        printf(\"CUDA Error: %s\\n\", cudaGetErrorString(err));\n",
    "    }\n",
    "\n",
    "    // 等待 GPU 完成\n",
    "    cudaDeviceSynchronize();\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a2f9c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['matmul_v2.cu', 'matmul_v2.so', '.virtual_documents', 'matmul_v3.cu', 'matmul_v3.so', 'matmul.cu', 'matmul.so']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "for v in os.listdir(os.path.abspath('.')):\n",
    "    prefix, end = os.path.splitext(v)\n",
    "    if end == '.cu':\n",
    "        subprocess.run(f\"nvcc -shared -o {prefix}.so {prefix}.cu -Xcompiler -fPIC\", shell=True)\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43a97eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在进行矩阵乘法测试: [1024x512] * [512x1024]\n",
      "Pytorch: \n",
      "348 µs ± 2.54 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "\n",
      "matmul_v2.so\n",
      "✅ 测试通过！结果正确。\n",
      "1.23 ms ± 6.18 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "\n",
      "正在进行矩阵乘法测试: [1024x512] * [512x1024]\n",
      "matmul_v3.so\n",
      "✅ 测试通过！结果正确。\n",
      "5.19 ms ± 6.15 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "正在进行矩阵乘法测试: [1024x512] * [512x1024]\n",
      "matmul.so\n",
      "✅ 测试通过！结果正确。\n",
      "2.23 ms ± 6.52 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import ctypes\n",
    "import numpy as np\n",
    "\n",
    "flag = True\n",
    "for v in os.listdir('.'):\n",
    "    prefix, end = os.path.splitext(v)\n",
    "    \n",
    "    if end != '.so':\n",
    "        continue\n",
    "\n",
    "    # 1. 加载编译好的 .so 库\n",
    "    lib = ctypes.CDLL(f'./{v}')\n",
    "\n",
    "    # 2. 定义函数参数类型\n",
    "    # void solve(const float* A, const float* B, float* C, int M, int N, int K)\n",
    "    # 指针对应 c_void_p (因为我们要传显存地址), int 对应 c_int\n",
    "    lib.solve.argtypes = [\n",
    "        ctypes.c_void_p, \n",
    "        ctypes.c_void_p, \n",
    "        ctypes.c_void_p, \n",
    "        ctypes.c_int, \n",
    "        ctypes.c_int, \n",
    "        ctypes.c_int\n",
    "    ]\n",
    "\n",
    "    def cuda_matmul(a_tensor, b_tensor):\n",
    "        # 获取维度\n",
    "        M, N = a_tensor.shape\n",
    "        N_b, K = b_tensor.shape\n",
    "        \n",
    "        assert N == N_b, f\"矩阵维度不匹配: {N} != {N_b}\"\n",
    "        \n",
    "        # 初始化输出矩阵 C (在 GPU 上分配)\n",
    "        c_tensor = torch.zeros((M, K), device='cuda', dtype=torch.float32)\n",
    "        \n",
    "        # 确保输入是连续内存且在 GPU 上\n",
    "        if not a_tensor.is_contiguous(): a_tensor = a_tensor.contiguous()\n",
    "        if not b_tensor.is_contiguous(): b_tensor = b_tensor.contiguous()\n",
    "        \n",
    "        # 3. 调用 CUDA 函数\n",
    "        # 注意：必须传入 data_ptr()，这是物理显存地址\n",
    "        lib.solve(\n",
    "            ctypes.c_void_p(a_tensor.data_ptr()),\n",
    "            ctypes.c_void_p(b_tensor.data_ptr()),\n",
    "            ctypes.c_void_p(c_tensor.data_ptr()),\n",
    "            ctypes.c_int(M),\n",
    "            ctypes.c_int(N),\n",
    "            ctypes.c_int(K)\n",
    "        )\n",
    "        \n",
    "        return c_tensor\n",
    "\n",
    "    # --- 测试部分 ---\n",
    "\n",
    "    # 设置维度\n",
    "    M, N, K = 1024, 512, 1024\n",
    "\n",
    "    print(f\"正在进行矩阵乘法测试: [{M}x{N}] * [{N}x{K}]\")\n",
    "\n",
    "    # 创建随机数据 (在 GPU 上)\n",
    "    A = torch.randn(M, N, device='cuda', dtype=torch.float32)\n",
    "    B = torch.randn(N, K, device='cuda', dtype=torch.float32)\n",
    "\n",
    "    # 1. 运行你的 CUDA Kernel\n",
    "    C_custom = cuda_matmul(A, B)\n",
    "\n",
    "    # 2. 运行 PyTorch 内置矩阵乘法 (作为标准答案)\n",
    "    C_torch = torch.matmul(A, B)\n",
    "    \n",
    "    if flag:\n",
    "        print(\"Pytorch: \")\n",
    "        %timeit torch.matmul(A, B); torch.cuda.synchronize()\n",
    "        flag = False\n",
    "        print()\n",
    "    print(f\"{v}\")\n",
    "    # 3. 验证结果\n",
    "    # 允许一点浮点误差\n",
    "    if torch.allclose(C_custom, C_torch, atol=1e-3):\n",
    "        print(f\"✅ 测试通过！结果正确。\")\n",
    "        %timeit cuda_matmul(A, B); torch.cuda.synchronize()\n",
    "    else:\n",
    "        print(\"❌ 测试失败。结果不一致。\")\n",
    "        print(\"最大误差:\", (C_custom - C_torch).abs().max().item())\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
